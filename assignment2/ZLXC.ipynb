{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:28:54.076767: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-16 15:28:54.162269: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-16 15:28:54.162330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-16 15:28:54.164381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-16 15:28:54.182365: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-16 15:28:55.797482: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score, accuracy_score\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(embedding_path):\n",
    "    raw_dataset = tf.data.TFRecordDataset([embedding_path])\n",
    "\n",
    "    for raw_record in raw_dataset.take(1):\n",
    "        example = tf.train.Example()\n",
    "\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "\n",
    "        embedding_feature = example.features.feature['embedding']\n",
    "\n",
    "        embedding_values = embedding_feature.float_list.value\n",
    "\n",
    "    return torch.tensor(embedding_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMIC_Embed_Dataset(Dataset):\n",
    "\n",
    "    pathologies = [\n",
    "        \"Enlarged Cardiomediastinum\",\n",
    "        \"Cardiomegaly\",\n",
    "        \"Lung Opacity\",\n",
    "        \"Lung Lesion\",\n",
    "        \"Edema\",\n",
    "        \"Consolidation\",\n",
    "        \"Pneumonia\",\n",
    "        \"Atelectasis\",\n",
    "        \"Pneumothorax\",\n",
    "        \"Pleural Effusion\",\n",
    "        \"Pleural Other\",\n",
    "        \"Fracture\",\n",
    "        \"Support Devices\",\n",
    "    ]\n",
    "\n",
    "    split_ratio = [0.8, 0.1, 0.1]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedpath,\n",
    "        csvpath,\n",
    "        metacsvpath,\n",
    "        views=[\"PA\"],\n",
    "        data_aug=None,\n",
    "        seed=0,\n",
    "        unique_patients=True,\n",
    "        mode=[\"train\", \"valid\", \"test\"][0],\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        np.random.seed(seed)  # Reset the seed so all runs are the same.\n",
    "\n",
    "        self.pathologies = sorted(self.pathologies)\n",
    "\n",
    "        self.mode = mode\n",
    "        self.embedpath = embedpath\n",
    "        self.data_aug = data_aug\n",
    "        self.csvpath = csvpath\n",
    "        self.csv = pd.read_csv(self.csvpath)\n",
    "        self.metacsvpath = metacsvpath\n",
    "        self.metacsv = pd.read_csv(self.metacsvpath)\n",
    "\n",
    "        self.csv = self.csv.set_index([\"subject_id\", \"study_id\"])\n",
    "        self.metacsv = self.metacsv.set_index([\"subject_id\", \"study_id\"])\n",
    "\n",
    "        self.csv = self.csv.join(self.metacsv).reset_index()\n",
    "\n",
    "        # Keep only the desired view\n",
    "        self.csv[\"view\"] = self.csv[\"ViewPosition\"]\n",
    "        self.limit_to_selected_views(views)\n",
    "\n",
    "        if unique_patients:\n",
    "            self.csv = self.csv.groupby(\"subject_id\").first().reset_index()\n",
    "\n",
    "        n_row = self.csv.shape[0]\n",
    "\n",
    "        # spit data to one of train valid test\n",
    "        if self.mode == \"train\":\n",
    "            self.csv = self.csv[: int(n_row * self.split_ratio[0])]\n",
    "        elif self.mode == \"valid\":\n",
    "            self.csv = self.csv[\n",
    "                int(n_row * self.split_ratio[0]) : int(\n",
    "                    n_row * (self.split_ratio[0] + self.split_ratio[1])\n",
    "                )\n",
    "            ]\n",
    "        elif self.mode == \"test\":\n",
    "            self.csv = self.csv[-int(n_row * self.split_ratio[-1]) :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"attr:mode has to be one of [train, valid, test] but your input is {self.mode}\"\n",
    "            )\n",
    "\n",
    "        # Get our classes.\n",
    "        healthy = self.csv[\"No Finding\"] == 1\n",
    "        labels = []\n",
    "        for pathology in self.pathologies:\n",
    "            if pathology in self.csv.columns:\n",
    "                self.csv.loc[healthy, pathology] = 0\n",
    "                mask = self.csv[pathology]\n",
    "\n",
    "            labels.append(mask.values)\n",
    "        self.labels = np.asarray(labels).T\n",
    "        self.labels = self.labels.astype(np.float32)\n",
    "\n",
    "        # Make all the -1 values into nans to keep things simple\n",
    "        self.labels[self.labels == -1] = np.nan\n",
    "\n",
    "        # Rename pathologies\n",
    "        self.pathologies = list(\n",
    "            np.char.replace(self.pathologies, \"Pleural Effusion\", \"Effusion\")\n",
    "        )\n",
    "\n",
    "        # add consistent csv values\n",
    "\n",
    "        # offset_day_int\n",
    "        self.csv[\"offset_day_int\"] = self.csv[\"StudyDate\"]\n",
    "\n",
    "        # patientid\n",
    "        self.csv[\"patientid\"] = self.csv[\"subject_id\"].astype(str)\n",
    "\n",
    "    def string(self):\n",
    "        return self.__class__.__name__ + \" num_samples={} views={}\".format(\n",
    "            len(self), self.views,\n",
    "        )\n",
    "\n",
    "    def limit_to_selected_views(self, views):\n",
    "        \"\"\"This function is called by subclasses to filter the\n",
    "        images by view based on the values in .csv['view']\n",
    "        \"\"\"\n",
    "        if type(views) is not list:\n",
    "            views = [views]\n",
    "        if '*' in views:\n",
    "            # if you have the wildcard, the rest are irrelevant\n",
    "            views = [\"*\"]\n",
    "        self.views = views\n",
    "\n",
    "        # missing data is unknown\n",
    "        self.csv.view.fillna(\"UNKNOWN\", inplace=True)\n",
    "\n",
    "        if \"*\" not in views:\n",
    "            self.csv = self.csv[self.csv[\"view\"].isin(self.views)]  # Select the view\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     sample = {}\n",
    "    #     sample[\"idx\"] = idx\n",
    "    #     sample[\"lab\"] = self.labels[idx]\n",
    "\n",
    "    #     subjectid = str(self.csv.iloc[idx][\"subject_id\"])\n",
    "    #     studyid = str(self.csv.iloc[idx][\"study_id\"])\n",
    "    #     dicom_id = str(self.csv.iloc[idx][\"dicom_id\"])\n",
    "\n",
    "\n",
    "    #     #data_aug\n",
    "    #     embed_file = os.path.join(\n",
    "    #         self.embedpath,\n",
    "    #         \"p\" + subjectid[:2],\n",
    "    #         \"p\" + subjectid,\n",
    "    #         \"s\" + studyid,\n",
    "    #         dicom_id + \".tfrecord\",\n",
    "    #     )\n",
    "    #     sample[\"embedding\"] = load_embedding(embed_file)\n",
    "    #     #sample[\"embedding\"] = embed_file\n",
    "\n",
    "    #     return sample\n",
    "    def __getitem__(self, idx):\n",
    "        subjectid = str(self.csv.iloc[idx][\"subject_id\"])\n",
    "        studyid = str(self.csv.iloc[idx][\"study_id\"])\n",
    "        dicom_id = str(self.csv.iloc[idx][\"dicom_id\"])\n",
    "\n",
    "        embed_file = os.path.join(\n",
    "            self.embedpath,\n",
    "            \"p\" + subjectid[:2],\n",
    "            \"p\" + subjectid,\n",
    "            \"s\" + studyid,\n",
    "            dicom_id + \".tfrecord\",\n",
    "        )\n",
    "        embedding = load_embedding(embed_file)     # shape: [1376]\n",
    "        label = self.labels[idx]                   # shape: [13]\n",
    "\n",
    "        return {\n",
    "            \"x\": embedding,   \n",
    "            \"lab\": label       \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_948587/4087053391.py:122: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.csv.view.fillna(\"UNKNOWN\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "embedpath = \"/d/hd04/armstrong/MIMIC/data/generalized-image-embeddings-for-the-mimic-chest-x-ray-dataset-1.0/files\"\n",
    "csvpath = \"/d/hd04/armstrong/MIMIC/data/mimic-cxr-2.0.0-chexpert.csv\"\n",
    "metacsvpath = \"/d/hd04/armstrong/MIMIC/data/mimic-cxr-2.0.0-metadata.csv\"\n",
    "\n",
    "dataset = MIMIC_Embed_Dataset(embedpath,csvpath,metacsvpath,mode = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 11:19:44.132466: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': tensor([-0.6009, -2.6448,  1.1589,  ...,  0.3574,  1.4271, -2.1083]),\n",
       " 'lab': array([nan,  1., nan, nan, nan,  1., nan,  1., nan, nan, nan,  1.,  0.],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset[1000]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "N = 36000\n",
    "subset = [dataset[i] for i in range(N)]\n",
    "train_len = int(N * 0.8)\n",
    "train_subset = subset[:train_len]\n",
    "val_subset = subset[train_len:]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = torch.stack([sample['x'] for sample in batch])  \n",
    "    y = torch.stack([torch.tensor(sample['lab']) for sample in batch])  \n",
    "    return {\n",
    "        \"x\": x,\n",
    "        \"y\": y\n",
    "    }\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "test_subset = [dataset[i] for i in range(N, N + 600)]\n",
    "test_loader = DataLoader(test_subset, batch_size=128, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "full_len = len(dataset)\n",
    "indices = list(range(full_len))\n",
    "train_len = int(full_len * 0.8)\n",
    "\n",
    "train_indices = indices[:train_len]\n",
    "val_indices = indices[train_len:]\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = torch.stack([sample['x'] for sample in batch])\n",
    "    y = torch.stack([torch.tensor(sample['lab']) for sample in batch])\n",
    "    return {\"x\": x, \"y\": y}\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True, collate_fn=collate_fn, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False, collate_fn=collate_fn, num_workers=8, pin_memory=True)\n",
    "\n",
    "test_indices = list(range(full_len - 600, full_len))\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_subset, batch_size=128, shuffle=False, collate_fn=collate_fn, num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureReformer(nn.Module):\n",
    "    def __init__(self, input_dim=1376, proj_dim=512, norm_type='layer', patchify=False, patch_num=16):\n",
    "        super().__init__()\n",
    "        self.patchify = patchify\n",
    "        self.patch_num = patch_num\n",
    "        self.proj_dim = proj_dim\n",
    "\n",
    "        if patchify:\n",
    "            assert proj_dim % patch_num == 0, \"proj_dim must be divisible by patch_num\"\n",
    "            self.patch_dim = proj_dim // patch_num\n",
    "        else:\n",
    "            self.patch_dim = proj_dim  \n",
    "\n",
    "        if norm_type == 'layer':\n",
    "            self.norm = nn.LayerNorm(self.patch_dim)\n",
    "        elif norm_type == 'batch':\n",
    "            self.norm = nn.BatchNorm1d(self.patch_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported norm_type: {norm_type}\")\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, proj_dim)\n",
    "        self.gate = nn.Linear(input_dim, proj_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):  # x: (B, D)\n",
    "        h = self.activation(self.fc(x))\n",
    "        g = self.sigmoid(self.gate(x))\n",
    "        z = h * g  # gated projection\n",
    "\n",
    "        if self.patchify:\n",
    "            # reshape: (B, patch_num, patch_dim)\n",
    "            z = z.view(z.size(0), self.patch_num, self.patch_dim)\n",
    "\n",
    "        if isinstance(self.norm, nn.BatchNorm1d):\n",
    "            if self.patchify:\n",
    "                B, N, D = z.shape\n",
    "                z = self.norm(z.view(B * N, D)).view(B, N, D)\n",
    "            else:\n",
    "                z = self.norm(z)\n",
    "        else:\n",
    "            z = self.norm(z)\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # [B, 1, D]\n",
    "        h = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = x + h\n",
    "        h = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + h\n",
    "        return x.squeeze(1)  \n",
    "\n",
    "class gMLPBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.sgu = nn.Linear(seq_len, seq_len)  # spatial gating\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        residual = x\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = x.transpose(1, 2)        # [B, C, L]\n",
    "        x = self.sgu(x)\n",
    "        x = x.transpose(1, 2)        # [B, L, C]\n",
    "        return self.fc2(x) + residual\n",
    "\n",
    "class SwinMLPBlock(nn.Module):\n",
    "    def __init__(self, input_dim, proj_dim=512, window_size=16,\n",
    "                 mlp_ratio=2.0, shift=False, debug=False,\n",
    "                 reform_input=True, flatten_output=True):\n",
    "        super().__init__()\n",
    "        self.reform_input = reform_input\n",
    "        self.flatten_output = flatten_output\n",
    "        self.shift = shift\n",
    "        self.debug = debug\n",
    "        self.window_size = window_size\n",
    "\n",
    "        if self.reform_input:\n",
    "            self.reformer = FeatureReformer(\n",
    "                input_dim=input_dim,\n",
    "                proj_dim=proj_dim,\n",
    "                norm_type='layer',\n",
    "                patchify=True,\n",
    "                patch_num=window_size  # patch_num = window_size\n",
    "            )\n",
    "            input_dim = proj_dim  \n",
    "\n",
    "        assert input_dim % window_size == 0, \"input_dim must be divisible by window_size\"\n",
    "        self.patch_dim = input_dim // window_size\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.patch_dim)\n",
    "        self.proj = nn.Linear(input_dim, input_dim)\n",
    "        self.gmlp = gMLPBlock(self.patch_dim, int(self.patch_dim * mlp_ratio), seq_len=window_size)\n",
    "\n",
    "    def forward(self, x):  # x: [B, input_dim] or [B, N, D]\n",
    "        if self.reform_input:\n",
    "            x = self.reformer(x)  # [B, N, D]\n",
    "            B, N, D = x.shape\n",
    "            x = x.view(B, -1) \n",
    "        B, D = x.shape\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[SwinMLPBlock] Input shape: {x.shape}\")\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = x.view(B, D // self.window_size, self.window_size)  # [B, N_win, patch_dim]\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"  â†’ Windowed shape: {x.shape}\")\n",
    "\n",
    "        if self.shift:\n",
    "            x = torch.roll(x, shifts=1, dims=1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.gmlp(x)\n",
    "\n",
    "        if self.shift:\n",
    "            x = torch.roll(x, shifts=-1, dims=1)\n",
    "\n",
    "        if self.flatten_output:\n",
    "            x = x.view(B, -1)  # [B, proj_dim]\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[SwinMLPBlock] Output shape: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "class WindowAttention1D(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [B, N, D]\n",
    "        h = x\n",
    "        x = self.norm(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        return h + self.dropout(x)  # [B, N, D]\n",
    "\n",
    "\n",
    "class CrossWindowAttention1D(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [B, N, D]\n",
    "        h = x\n",
    "        x = self.norm(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        return h + self.dropout(x)  # [B, N, D]\n",
    "\n",
    "\n",
    "class MlTrBlock1D(nn.Module):\n",
    "    def __init__(self, input_dim=1376, proj_dim=512, patch_num=16,\n",
    "                 num_heads=4, dropout=0.1, ffn_ratio=2.0,\n",
    "                 reform_input=True, flatten_output=True):\n",
    "        super().__init__()\n",
    "        self.reform_input = reform_input\n",
    "        self.flatten_output = flatten_output\n",
    "\n",
    "        if self.reform_input:\n",
    "            self.reformer = FeatureReformer(\n",
    "                input_dim=input_dim,\n",
    "                proj_dim=proj_dim,\n",
    "                norm_type='layer',\n",
    "                patchify=True,\n",
    "                patch_num=patch_num\n",
    "            )\n",
    "\n",
    "        patch_dim = proj_dim // patch_num\n",
    "        self.patch_num = patch_num\n",
    "        self.patch_dim = patch_dim\n",
    "\n",
    "        self.window_attn = WindowAttention1D(patch_dim, num_heads, dropout)\n",
    "        self.cross_attn = CrossWindowAttention1D(patch_dim, num_heads, dropout)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, int(patch_dim * ffn_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(patch_dim * ffn_ratio), patch_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: [B, input_dim] or [B, N_win, patch_dim]\n",
    "        if self.reform_input:\n",
    "            assert x.shape[1] == self.reformer.fc.in_features, \\\n",
    "                f\"Input dim mismatch: got {x.shape[1]}, expected {self.reformer.fc.in_features}\"\n",
    "            x = self.reformer(x)  # [B, patch_num, patch_dim]\n",
    "\n",
    "        x = self.window_attn(x)\n",
    "        x = self.cross_attn(x)\n",
    "\n",
    "        residual = x\n",
    "        B, N, D = x.shape\n",
    "        x = self.ffn(x.view(B * N, D)).view(B, N, D)\n",
    "        x = x + residual\n",
    "\n",
    "        return x.view(x.size(0), -1) if self.flatten_output else x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_type=\"vanilla\",\n",
    "                 num_layers=2,\n",
    "                 return_all_layers=False,\n",
    "                 **block_kwargs):\n",
    "        super().__init__()\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        block_map = {\n",
    "            \"vanilla\": VanillaTransformerBlock,\n",
    "            \"swinmlp\": SwinMLPBlock,\n",
    "            \"mltr\": MlTrBlock1D,\n",
    "        }\n",
    "\n",
    "        encoder_type = encoder_type.lower()\n",
    "        if encoder_type not in block_map:\n",
    "            raise ValueError(f\"Unsupported encoder_type: {encoder_type}\")\n",
    "\n",
    "        block_class = block_map[encoder_type]\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        if encoder_type in [\"mltr\", \"swinmlp\"]:\n",
    "            for i in range(num_layers):\n",
    "                self.layers.append(block_class(\n",
    "                    reform_input=(i == 0),\n",
    "                    flatten_output=(i == num_layers - 1),\n",
    "                    **block_kwargs\n",
    "                ))\n",
    "        else:\n",
    "            block = partial(block_class, **block_kwargs)\n",
    "            self.layers.extend([block() for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if self.return_all_layers:\n",
    "                outputs.append(x)\n",
    "        return outputs if self.return_all_layers else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_label_cooccurrence(dataset, num_labels):\n",
    "    co_matrix = np.zeros((num_labels, num_labels))\n",
    "\n",
    "    for sample in dataset:\n",
    "        labels = sample['lab']\n",
    "        if labels is None:\n",
    "            continue\n",
    "        binary_mask = ~np.isnan(labels)\n",
    "        present_labels = np.where(labels == 1)[0]\n",
    "        for i in present_labels:\n",
    "            for j in present_labels:\n",
    "                if i != j:\n",
    "                    co_matrix[i, j] += 1\n",
    "    return co_matrix\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    D = np.diag(np.power(A.sum(axis=1) + 1e-5, -0.5))\n",
    "    return D @ A @ D\n",
    "\n",
    "def build_label_graph(dataset, num_labels):\n",
    "    co_matrix = build_label_cooccurrence(dataset, num_labels)\n",
    "    A = normalize_adjacency(co_matrix)\n",
    "    return torch.tensor(A, dtype=torch.float32)\n",
    "\n",
    "class LabelGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, A):\n",
    "        super().__init__()\n",
    "        self.A = A  # [num_labels, num_labels]\n",
    "        self.gcn1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.gcn2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, label_embed):  \n",
    "        x = torch.matmul(self.A, label_embed)\n",
    "        x = F.relu(self.gcn1(x))\n",
    "        x = torch.matmul(self.A, x)\n",
    "        x = self.gcn2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query2LabelDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_labels=13, num_heads=4, num_layers=2, dropout=0.1, use_norm=True):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_norm = use_norm\n",
    "\n",
    "        self.label_queries = nn.Parameter(torch.randn(num_labels, embed_dim))\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim) if use_norm else nn.Identity()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_feats, label_embed):\n",
    "        B = encoder_feats.size(0)\n",
    "\n",
    "        if encoder_feats.dim() == 2:\n",
    "            encoder_feats = encoder_feats.unsqueeze(1)  # [B, 1, d]\n",
    "\n",
    "        queries = label_embed.unsqueeze(0).expand(B, -1, -1)  # [B, L, d]\n",
    "\n",
    "        out = self.decoder(tgt=queries, memory=encoder_feats)  # [B, L, d]\n",
    "        out = self.norm(out)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.classifier(out).squeeze(-1)  # [B, L]\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, num_labels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x).squeeze(-1)  # (B, L)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "def masked_bce_loss(pred, target, eps=1e-8):\n",
    "    mask = ~torch.isnan(target)                            \n",
    "    target_clean = torch.nan_to_num(target, nan=0.0)    \n",
    "    loss_fn = nn.BCELoss(reduction='none')            \n",
    "    loss = loss_fn(pred, target_clean)                   \n",
    "    return (loss * mask).sum() / (mask.sum() + eps)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EndToEndModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=1376,\n",
    "                 proj_dim=512,\n",
    "                 num_labels=13,\n",
    "                 A=None,\n",
    "                 norm_type='layer',\n",
    "                 patchify=False,\n",
    "                 patch_num=16,\n",
    "                 encoder_type=\"vanilla\",\n",
    "                 encoder_layers=3,\n",
    "                 decoder_layers=3,\n",
    "                 num_heads=4,\n",
    "                 use_label_gcn=True,\n",
    "                 output_mode=\"logits\"  # or \"features\"\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patchify = patchify\n",
    "        self.encoder_type = encoder_type.lower()\n",
    "        self.output_mode = output_mode\n",
    "        self.use_label_gcn = use_label_gcn\n",
    "\n",
    "        incompatible_patch_structures = [\"mltr\", \"swinmlp\"]\n",
    "        if not patchify and self.encoder_type in incompatible_patch_structures:\n",
    "            raise ValueError(f\"Encoder '{encoder_type}' requires patchify=True, but you set patchify=False.\")\n",
    "\n",
    "        if self.encoder_type == \"vanilla\":\n",
    "            self.reformer = FeatureReformer(\n",
    "                input_dim=input_dim,\n",
    "                proj_dim=proj_dim,\n",
    "                norm_type=norm_type,\n",
    "                patchify=patchify,\n",
    "                patch_num=patch_num\n",
    "            )\n",
    "            encoder_kwargs = {\n",
    "                \"embed_dim\": proj_dim,\n",
    "                \"num_heads\": num_heads\n",
    "            }\n",
    "\n",
    "        elif self.encoder_type == \"swinmlp\":\n",
    "            encoder_kwargs = {\n",
    "                \"input_dim\": proj_dim,     \n",
    "                \"window_size\": patch_num\n",
    "            }\n",
    "\n",
    "        elif self.encoder_type == \"mltr\":\n",
    "            encoder_kwargs = {\n",
    "                \"input_dim\": input_dim, \n",
    "                \"proj_dim\": proj_dim,\n",
    "                \"patch_num\": patch_num,\n",
    "                \"num_heads\": num_heads\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported encoder_type: {encoder_type}\")\n",
    "\n",
    "        self.encoder = TransformerEncoderBlock(\n",
    "            encoder_type=self.encoder_type,\n",
    "            num_layers=encoder_layers,\n",
    "            return_all_layers=False,\n",
    "            **encoder_kwargs\n",
    "        )\n",
    "\n",
    "        if self.use_label_gcn:\n",
    "            assert A is not None, \"Adjacency matrix A must be provided for LabelGCN\"\n",
    "            self.label_gcn = LabelGCN(\n",
    "                in_dim=proj_dim,\n",
    "                hidden_dim=proj_dim,\n",
    "                out_dim=proj_dim,\n",
    "                A=A\n",
    "            )\n",
    "\n",
    "        self.decoder = Query2LabelDecoder(\n",
    "            embed_dim=proj_dim,\n",
    "            num_labels=num_labels,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=decoder_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x, label_embed=None):\n",
    "        assert label_embed is not None, \"label_embed must be provided\"\n",
    "\n",
    "        if self.encoder_type == \"vanilla\":\n",
    "            x = self.reformer(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        if self.use_label_gcn:\n",
    "            label_embed = self.label_gcn(label_embed)\n",
    "\n",
    "        probs = self.decoder(x, label_embed)\n",
    "\n",
    "        if self.output_mode == \"logits\":\n",
    "            return probs\n",
    "        elif self.output_mode == \"features\":\n",
    "            return x, probs\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output_mode: {self.output_mode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, average_precision_score\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, patience=5, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = masked_bce_loss\n",
    "        self.device = device\n",
    "        self.best_model = None\n",
    "        self.best_val_auc = -np.inf\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_scores = None  \n",
    "\n",
    "    def evaluate(self, dataloader, label_embed, pathologies=None):\n",
    "        self.model.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                x = batch['x'].to(self.device)\n",
    "                y = batch['y'].to(self.device)\n",
    "\n",
    "                preds = self.model(x, label_embed=label_embed)\n",
    "                all_preds.append(preds.detach().cpu())\n",
    "                all_targets.append(y.detach().cpu())\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        num_labels = all_targets.shape[1]\n",
    "        aucs, f1s, maps, accs = [], [], [], []\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            mask = ~torch.isnan(all_targets[:, i])\n",
    "            if mask.sum() < 10:\n",
    "                aucs.append(None)\n",
    "                f1s.append(None)\n",
    "                maps.append(None)\n",
    "                accs.append(None)\n",
    "                continue\n",
    "\n",
    "            y_true = all_targets[:, i][mask].numpy()\n",
    "            y_prob = all_preds[:, i][mask].numpy()\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "            aucs.append(roc_auc_score(y_true, y_prob))\n",
    "            f1s.append(f1_score(y_true, y_pred))\n",
    "            maps.append(average_precision_score(y_true, y_prob))\n",
    "            accs.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "        valid_aucs = [a for a in aucs if a is not None]\n",
    "        macro_auc = sum(valid_aucs) / len(valid_aucs)\n",
    "\n",
    "        index = pathologies if pathologies is not None else [f\"Label_{i}\" for i in range(num_labels)]\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'AUC': aucs,\n",
    "            'F1': f1s,\n",
    "            'mAP': maps,\n",
    "            'Accuracy': accs\n",
    "        }, index=index).round(4)\n",
    "\n",
    "        return macro_auc, metrics_df  \n",
    "\n",
    "    def train(self, train_loader, val_loader, label_embed_init, epochs=50, pathologies=None):\n",
    "        self.model.train()\n",
    "        label_embed_init = label_embed_init.to(self.device)\n",
    "\n",
    "        loss_list, auc_list = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch in train_loader:\n",
    "                x = batch['x'].to(self.device)\n",
    "                y = batch['y'].to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.model(x, label_embed=label_embed_init)\n",
    "                loss = self.criterion(preds, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            loss_list.append(epoch_loss / len(train_loader))\n",
    "\n",
    "            val_auc, val_df = self.evaluate(val_loader, label_embed_init, pathologies)\n",
    "            auc_list.append(val_auc)\n",
    "\n",
    "            print(f\"Epoch {epoch+1} | Loss: {loss_list[-1]:.4f} | AUC: {val_auc:.4f}\")\n",
    "            display(val_df)\n",
    "\n",
    "            if val_auc > self.best_val_auc:\n",
    "                self.best_val_auc = val_auc\n",
    "                self.best_model = copy.deepcopy(self.model.state_dict())\n",
    "                self.best_scores = val_df\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        if self.best_model is not None:\n",
    "            self.model.load_state_dict(self.best_model)\n",
    "\n",
    "        return self.model, self.best_scores, loss_list, auc_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "input_dim = 1376\n",
    "proj_dim = 512\n",
    "num_labels = 13\n",
    "patchify = True # mltr\n",
    "# patchify = False # vanilla \n",
    "patch_num = 16\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "A = build_label_graph(train_loader.dataset, num_labels).to(device)\n",
    "\n",
    "label_embed_init = torch.randn(num_labels, proj_dim).to(device)\n",
    "\n",
    "model = EndToEndModel(\n",
    "    input_dim=input_dim,\n",
    "    proj_dim=proj_dim,\n",
    "    num_labels=num_labels,\n",
    "    A=A,\n",
    "    norm_type='layer',\n",
    "    patchify=patchify,\n",
    "    patch_num=patch_num,\n",
    "    encoder_type=\"mltr\",          # vanilla, mltr\n",
    "    encoder_layers=2,\n",
    "    decoder_layers=2,\n",
    "    num_heads=4,\n",
    "    use_label_gcn=True,\n",
    "    output_mode=\"logits\"\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "trainer = Trainer(model, optimizer, patience=10, device=device)\n",
    "\n",
    "model, metrics_df, loss_list, auc_list = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    label_embed_init=label_embed_init,\n",
    "    epochs=100,\n",
    "    pathologies=dataset.pathologies\n",
    ")\n",
    "\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), \"end2end_model.pt\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_list, label=\"EndToEndModel\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(auc_list, label=\"EndToEndModel\")\n",
    "plt.title(\"Validation AUC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(test_loader, label_embed_init)\n",
    "print(\"Test Set Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_list, label=\"EndToEndModel\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(auc_list, label=\"EndToEndModel\")\n",
    "plt.title(\"Validation AUC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "results = {}  \n",
    "input_dim = 1376\n",
    "proj_dim = 512\n",
    "num_labels = 13\n",
    "# patchify = True\n",
    "# patchify = False\n",
    "patch_num = 16\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "A = build_label_graph(train_loader.dataset, num_labels).to(device)\n",
    "\n",
    "label_embed_init = torch.randn(num_labels, proj_dim).to(device)\n",
    "\n",
    "for encoder_type in [\"vanilla\", \"mltr\"]:\n",
    "    print(f\"\\nTraining model with encoder: {encoder_type}\")\n",
    "\n",
    "    patchify = True if encoder_type == \"mltr\" else False \n",
    "\n",
    "    model = EndToEndModel(\n",
    "        input_dim=input_dim,\n",
    "        proj_dim=proj_dim,\n",
    "        num_labels=num_labels,\n",
    "        A=A,\n",
    "        norm_type='layer',\n",
    "        patchify=patchify,\n",
    "        patch_num=patch_num,\n",
    "        encoder_type=encoder_type,\n",
    "        encoder_layers=2,\n",
    "        decoder_layers=2,\n",
    "        num_heads=4,\n",
    "        use_label_gcn=True,\n",
    "        output_mode=\"logits\"\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    trainer = Trainer(model, optimizer, patience=5, device=device)\n",
    "\n",
    "    model, metrics_df, loss_list, auc_list = trainer.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        label_embed_init=label_embed_init,\n",
    "        epochs=30,\n",
    "        pathologies=dataset.pathologies\n",
    "    )\n",
    "\n",
    "    results[encoder_type] = {\n",
    "        \"loss\": loss_list,\n",
    "        \"auc\": auc_list,\n",
    "        \"df\": metrics_df\n",
    "    }\n",
    "\n",
    "    print(f\"Finished training: {encoder_type}\")\n",
    "    display(metrics_df)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for name in results:\n",
    "    plt.plot(results[name][\"loss\"], label=f\"{name}\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name in results:\n",
    "    plt.plot(results[name][\"auc\"], label=f\"{name}\")\n",
    "plt.title(\"Validation AUC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtrain",
   "language": "python",
   "name": "llmtrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
