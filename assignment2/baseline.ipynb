{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgk2UnxFufJA"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xazJlKM4ugi9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mqTjKEPrumA6"
      },
      "outputs": [],
      "source": [
        "def load_embedding(embedding_path):\n",
        "    raw_dataset = tf.data.TFRecordDataset([embedding_path])\n",
        "    for raw_record in raw_dataset.take(1):\n",
        "        example = tf.train.Example()\n",
        "        example.ParseFromString(raw_record.numpy())\n",
        "        embedding_feature = example.features.feature['embedding']\n",
        "        embedding_values = embedding_feature.float_list.value\n",
        "    return torch.tensor(embedding_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "evknnsPOumIl"
      },
      "outputs": [],
      "source": [
        "class MIMIC_Embed_Dataset(Dataset):\n",
        "\n",
        "    pathologies = [\n",
        "        \"Enlarged Cardiomediastinum\",\n",
        "        \"Cardiomegaly\",\n",
        "        \"Lung Opacity\",\n",
        "        \"Lung Lesion\",\n",
        "        \"Edema\",\n",
        "        \"Consolidation\",\n",
        "        \"Pneumonia\",\n",
        "        \"Atelectasis\",\n",
        "        \"Pneumothorax\",\n",
        "        \"Pleural Effusion\",\n",
        "        \"Pleural Other\",\n",
        "        \"Fracture\",\n",
        "        \"Support Devices\",\n",
        "    ]\n",
        "\n",
        "    split_ratio = [0.8, 0.1, 0.1]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedpath,\n",
        "        csvpath,\n",
        "        metacsvpath,\n",
        "        views=[\"PA\"],\n",
        "        data_aug=None,\n",
        "        seed=0,\n",
        "        unique_patients=True,\n",
        "        mode=[\"train\", \"valid\", \"test\"][0],\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        np.random.seed(seed) \n",
        "\n",
        "        self.pathologies = sorted(self.pathologies)\n",
        "\n",
        "        self.mode = mode\n",
        "        self.embedpath = embedpath\n",
        "        self.data_aug = data_aug\n",
        "        self.csvpath = csvpath\n",
        "        self.csv = pd.read_csv(self.csvpath)\n",
        "        self.metacsvpath = metacsvpath\n",
        "        self.metacsv = pd.read_csv(self.metacsvpath)\n",
        "\n",
        "        self.csv = self.csv.set_index([\"subject_id\", \"study_id\"])\n",
        "        self.metacsv = self.metacsv.set_index([\"subject_id\", \"study_id\"])\n",
        "\n",
        "        self.csv = self.csv.join(self.metacsv).reset_index()\n",
        "\n",
        "        # Keep only the desired view\n",
        "        self.csv[\"view\"] = self.csv[\"ViewPosition\"]\n",
        "        self.limit_to_selected_views(views)\n",
        "\n",
        "        if unique_patients:\n",
        "            self.csv = self.csv.groupby(\"subject_id\").first().reset_index()\n",
        "\n",
        "        n_row = self.csv.shape[0]\n",
        "\n",
        "        # spit data to one of train valid test\n",
        "        if self.mode == \"train\":\n",
        "            self.csv = self.csv[: int(n_row * self.split_ratio[0])]\n",
        "        elif self.mode == \"valid\":\n",
        "            self.csv = self.csv[\n",
        "                int(n_row * self.split_ratio[0]) : int(\n",
        "                    n_row * (self.split_ratio[0] + self.split_ratio[1])\n",
        "                )\n",
        "            ]\n",
        "        elif self.mode == \"test\":\n",
        "            self.csv = self.csv[-int(n_row * self.split_ratio[-1]) :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"attr:mode has to be one of [train, valid, test] but your input is {self.mode}\"\n",
        "            )\n",
        "\n",
        "        # Get our classes.\n",
        "        healthy = self.csv[\"No Finding\"] == 1\n",
        "        labels = []\n",
        "        for pathology in self.pathologies:\n",
        "            if pathology in self.csv.columns:\n",
        "                self.csv.loc[healthy, pathology] = 0\n",
        "                mask = self.csv[pathology]\n",
        "\n",
        "            labels.append(mask.values)\n",
        "        self.labels = np.asarray(labels).T\n",
        "        self.labels = self.labels.astype(np.float32)\n",
        "\n",
        "        # Make all the -1 values into nans to keep things simple\n",
        "        self.labels[self.labels == -1] = np.nan\n",
        "\n",
        "        # Rename pathologies\n",
        "        self.pathologies = list(\n",
        "            np.char.replace(self.pathologies, \"Pleural Effusion\", \"Effusion\")\n",
        "        )\n",
        "\n",
        "        # add consistent csv values\n",
        "\n",
        "        # offset_day_int\n",
        "        self.csv[\"offset_day_int\"] = self.csv[\"StudyDate\"]\n",
        "\n",
        "        # patientid\n",
        "        self.csv[\"patientid\"] = self.csv[\"subject_id\"].astype(str)\n",
        "\n",
        "    def string(self):\n",
        "        return self.__class__.__name__ + \" num_samples={} views={}\".format(\n",
        "            len(self), self.views,\n",
        "        )\n",
        "\n",
        "    def limit_to_selected_views(self, views):\n",
        "        \"\"\"This function is called by subclasses to filter the\n",
        "        images by view based on the values in .csv['view']\n",
        "        \"\"\"\n",
        "        if type(views) is not list:\n",
        "            views = [views]\n",
        "        if '*' in views:\n",
        "            # if you have the wildcard, the rest are irrelevant\n",
        "            views = [\"*\"]\n",
        "        self.views = views\n",
        "\n",
        "        # missing data is unknown\n",
        "        self.csv.view.fillna(\"UNKNOWN\", inplace=True)\n",
        "\n",
        "        if \"*\" not in views:\n",
        "            self.csv = self.csv[self.csv[\"view\"].isin(self.views)]  # Select the view\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {}\n",
        "        sample[\"idx\"] = idx\n",
        "        sample[\"lab\"] = self.labels[idx]\n",
        "\n",
        "        subjectid = str(self.csv.iloc[idx][\"subject_id\"])\n",
        "        studyid = str(self.csv.iloc[idx][\"study_id\"])\n",
        "        dicom_id = str(self.csv.iloc[idx][\"dicom_id\"])\n",
        "\n",
        "\n",
        "        #data_aug\n",
        "        embed_file = os.path.join(\n",
        "            self.embedpath,\n",
        "            \"p\" + subjectid[:2],\n",
        "            \"p\" + subjectid,\n",
        "            \"s\" + studyid,\n",
        "            dicom_id + \".tfrecord\",\n",
        "        )\n",
        "        sample[\"embedding\"] = load_embedding(embed_file)\n",
        "        #sample[\"embedding\"] = embed_file\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUkna9bZumK-",
        "outputId": "311905e4-4e18-4a75-c7ac-8f9eee8a4bc3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_310336/420719509.py:122: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  self.csv.view.fillna(\"UNKNOWN\", inplace=True)\n"
          ]
        }
      ],
      "source": [
        "embedpath = \"/d/hd04/armstrong/MIMIC/data/generalized-image-embeddings-for-the-mimic-chest-x-ray-dataset-1.0/files\"\n",
        "csvpath = \"/d/hd04/armstrong/MIMIC/data/mimic-cxr-2.0.0-chexpert.csv\"\n",
        "metacsvpath = \"/d/hd04/armstrong/MIMIC/data/mimic-cxr-2.0.0-metadata.csv\"\n",
        "\n",
        "dataset = MIMIC_Embed_Dataset(embedpath,csvpath,metacsvpath,mode = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6C3lDKczEJ7",
        "outputId": "672d9141-463b-4eef-e86f-ac8f7a7a9b27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-16 11:16:48.742988: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'idx': 1000,\n",
              " 'lab': array([nan,  1., nan, nan, nan,  1., nan,  1., nan, nan, nan,  1.,  0.],\n",
              "       dtype=float32),\n",
              " 'embedding': tensor([-0.6009, -2.6448,  1.1589,  ...,  0.3574,  1.4271, -2.1083])}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample = dataset[1000]\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETRbtSAZ4qqt"
      },
      "source": [
        "# baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v23iFaGkXTt3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "N = 36000\n",
        "subset = [dataset[i] for i in range(N)]\n",
        "train_len = int(N * 0.8)\n",
        "train_subset = subset[:train_len]\n",
        "val_subset = subset[train_len:]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    x = torch.stack([sample['embedding'] for sample in batch])\n",
        "    y = torch.stack([torch.tensor(sample['lab']) for sample in batch])\n",
        "    return {\"x\": x, \"y\": y}\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "test_subset = [dataset[i] for i in range(N, N + 500)]\n",
        "test_loader = DataLoader(test_subset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "N = 36000\n",
        "train_indices = list(range(0, int(N * 0.8)))\n",
        "val_indices = list(range(int(N * 0.8), N))\n",
        "test_indices = list(range(N, N + 500))\n",
        "\n",
        "train_subset = Subset(dataset, train_indices)\n",
        "val_subset = Subset(dataset, val_indices)\n",
        "test_subset = Subset(dataset, test_indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True,\n",
        "                          collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False,\n",
        "                        collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=128, shuffle=False,\n",
        "                         collate_fn=collate_fn, num_workers=4, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13Z075g3XT6U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score, accuracy_score\n",
        "import pandas as pd\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "# MLP\n",
        "class FlexibleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, norm='batch', dropout=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, h_dim))\n",
        "            if norm == 'batch':\n",
        "                layers.append(nn.BatchNorm1d(h_dim))\n",
        "            elif norm == 'layer':\n",
        "                layers.append(nn.LayerNorm(h_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            prev_dim = h_dim\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Transformer\n",
        "class TransformerMultiModalClassifier(nn.Module):\n",
        "    def __init__(self, image_input_dim=1376, patch_size=8, embed_dim=256,\n",
        "                 num_heads=4, num_layers=2, output_dim=13, dropout=0.1,\n",
        "                 meta_dim=0, use_positional_encoding=True,\n",
        "                 norm='batch'):\n",
        "        super().__init__()\n",
        "        assert image_input_dim % patch_size == 0, \"input_dim must be divisible by patch_size\"\n",
        "        self.seq_len = image_input_dim // patch_size\n",
        "        self.use_positional_encoding = use_positional_encoding\n",
        "\n",
        "        self.patch_embed = nn.Linear(patch_size, embed_dim)\n",
        "        if use_positional_encoding:\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(1, self.seq_len, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads,\n",
        "            dim_feedforward=embed_dim * 2, dropout=dropout, batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        total_input_dim = embed_dim * self.seq_len + meta_dim\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(total_input_dim, 128),\n",
        "            nn.BatchNorm1d(128) if norm == 'batch' else nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_meta=None):\n",
        "        B = x_img.size(0)\n",
        "        x = x_img.view(B, self.seq_len, -1)\n",
        "        x = self.patch_embed(x)\n",
        "        if self.use_positional_encoding:\n",
        "            x = x + self.pos_embedding\n",
        "        x = self.transformer(x)\n",
        "        x_flat = x.flatten(1)\n",
        "        if x_meta is not None:\n",
        "            x = torch.cat([x_flat, x_meta], dim=1)\n",
        "        else:\n",
        "            x = x_flat\n",
        "        return self.head(x)\n",
        "\n",
        "# ResMLP\n",
        "class ResidualMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=4, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim, hidden_dim)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_in(x)\n",
        "        for layer in self.layers:\n",
        "            x = x + layer(x)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "# TransformerMLP \n",
        "class TransformerMLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1376, output_dim=13, hidden_dim=512, dropout=0.1, d_token=64, n_layers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.linear_in = nn.Linear(input_dim, d_token)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_token, nhead=n_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(d_token),\n",
        "            nn.Linear(d_token, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_in(x).unsqueeze(1)\n",
        "        x = self.transformer(x).squeeze(1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# ViT-like\n",
        "class ViTLikeClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1376, output_dim=13, patch_size=32, hidden_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert input_dim % patch_size == 0, \"input_dim must be divisible by patch_size\"\n",
        "        self.n_patches = input_dim // patch_size\n",
        "        self.patch_embed = nn.Linear(patch_size, hidden_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = x.view(B, self.n_patches, -1)  # (B, num_patches, patch_size)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = self.transformer(x)\n",
        "        return self.classifier(x[:, 0])\n",
        "\n",
        "\n",
        "# FTTransformer\n",
        "class FTTransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1376, output_dim=13, hidden_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(input_dim, hidden_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.embed(x).unsqueeze(1)  # (B, 1, hidden_dim)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = self.transformer(x)\n",
        "        return self.classifier(x[:, 0])\n",
        "\n",
        "# Swin-MLP\n",
        "class SwinMLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1376, output_dim=13, hidden_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x + self.mlp(self.norm(x))  # Swin-style MLP block with skip connection\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# SAINT\n",
        "class SAINTClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1376, output_dim=13, hidden_dim=512, dropout=0.1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(input_dim, hidden_dim)\n",
        "        self.row_attn = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads, dropout=dropout, batch_first=True), num_layers=1)\n",
        "        self.col_attn = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads, dropout=dropout, batch_first=True), num_layers=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x).unsqueeze(1)  # (B, 1, D)\n",
        "        x = self.row_attn(x)\n",
        "        x = self.col_attn(x)\n",
        "        return self.classifier(x.squeeze(1))\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, pathologies, test_loader=None,\n",
        "                epochs=10, lr=1e-3, device=None, early_stop_patience=3,\n",
        "                return_metrics=False, model_name=\"Model\"):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
        "    writer = SummaryWriter(log_dir=f'runs/{model_name}')\n",
        "\n",
        "    train_losses, val_aucs = [], []\n",
        "    best_auc, patience, best_model = 0, 0, None\n",
        "    best_scores = {}\n",
        "\n",
        "    print(f\"\\nTraining [{model_name}]...\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            x = batch['x'].to(device)\n",
        "            y = batch['y'].to(device)\n",
        "            mask = ~torch.isnan(y)\n",
        "            y_clean = torch.nan_to_num(y, nan=0.0)\n",
        "\n",
        "            logits = model(x)\n",
        "            loss_all = loss_fn(logits, y_clean)\n",
        "            loss = (loss_all * mask).sum() / mask.sum()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
        "        print(f\"Epoch {epoch}/{epochs} | Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        y_true, y_prob = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x = batch['x'].to(device)\n",
        "                y = batch['y'].cpu().numpy()\n",
        "                logits = model(x).cpu()\n",
        "                probs = torch.sigmoid(logits).numpy()\n",
        "                y_true.append(y)\n",
        "                y_prob.append(probs)\n",
        "\n",
        "        y_true = np.concatenate(y_true, axis=0)\n",
        "        y_prob = np.concatenate(y_prob, axis=0)\n",
        "\n",
        "        aucs, f1s, maps, accs = [], [], [], []\n",
        "        for i, name in enumerate(pathologies):\n",
        "            mask = ~np.isnan(y_true[:, i])\n",
        "            if mask.sum() < 10:\n",
        "                continue\n",
        "            y_true_i, y_prob_i = y_true[mask, i], y_prob[mask, i]\n",
        "            y_pred_i = (y_prob_i > 0.5).astype(int)\n",
        "\n",
        "            aucs.append(roc_auc_score(y_true_i, y_prob_i))\n",
        "            f1s.append(f1_score(y_true_i, y_pred_i))\n",
        "            maps.append(average_precision_score(y_true_i, y_prob_i))\n",
        "            accs.append(accuracy_score(y_true_i, y_pred_i))\n",
        "\n",
        "        val_auc = np.mean(aucs)\n",
        "        val_aucs.append(val_auc)\n",
        "        writer.add_scalar(\"AUC/val\", val_auc, epoch)\n",
        "        print(f\"Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            patience = 0\n",
        "            best_model = model.state_dict()\n",
        "            best_scores = {\n",
        "                'AUC': aucs,\n",
        "                'F1': f1s,\n",
        "                'mAP': maps,\n",
        "                'Accuracy': accs\n",
        "            }\n",
        "            torch.save(best_model, f\"{model_name}_best.pt\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= early_stop_patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    if best_model:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    metrics_df = pd.DataFrame(best_scores, index=pathologies[:len(best_scores['AUC'])]).round(4)\n",
        "\n",
        "    if test_loader:\n",
        "        print(f\"\\nEvaluating [{model_name}] on Test Set...\")\n",
        "        model.eval()\n",
        "        y_true, y_prob = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                x = batch['x'].to(device)\n",
        "                y = batch['y'].cpu().numpy()\n",
        "                logits = model(x).cpu()\n",
        "                probs = torch.sigmoid(logits).numpy()\n",
        "                y_true.append(y)\n",
        "                y_prob.append(probs)\n",
        "\n",
        "        y_true = np.concatenate(y_true, axis=0)\n",
        "        y_prob = np.concatenate(y_prob, axis=0)\n",
        "\n",
        "        test_aucs, test_f1s, test_maps, test_accs = [], [], [], []\n",
        "        for i, name in enumerate(pathologies):\n",
        "            mask = ~np.isnan(y_true[:, i])\n",
        "            if mask.sum() < 10:\n",
        "                continue\n",
        "            y_true_i, y_prob_i = y_true[mask, i], y_prob[mask, i]\n",
        "            y_pred_i = (y_prob_i > 0.5).astype(int)\n",
        "            test_aucs.append(roc_auc_score(y_true_i, y_prob_i))\n",
        "            test_f1s.append(f1_score(y_true_i, y_pred_i))\n",
        "            test_maps.append(average_precision_score(y_true_i, y_prob_i))\n",
        "            test_accs.append(accuracy_score(y_true_i, y_pred_i))\n",
        "\n",
        "        test_df = pd.DataFrame({\n",
        "            'AUC': test_aucs,\n",
        "            'F1': test_f1s,\n",
        "            'MAP': test_maps,\n",
        "            'Accuracy': test_accs\n",
        "        }, index=pathologies[:len(test_aucs)]).round(4)\n",
        "        print(test_df)\n",
        "\n",
        "    if return_metrics:\n",
        "        return train_losses, val_aucs, metrics_df\n",
        "    else:\n",
        "        display(metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydt1oaYpfxu6"
      },
      "outputs": [],
      "source": [
        "def plot_model_comparison(loss_dict, auc_dict):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for name, losses in loss_dict.items():\n",
        "        plt.plot(losses, label=name)\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for name, aucs in auc_dict.items():\n",
        "        plt.plot(aucs, label=name)\n",
        "    plt.title(\"Validation AUC\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "model_constructors = OrderedDict({\n",
        "    \"MLP\": lambda hidden_dim, dropout, **kwargs: FlexibleMLP(input_dim=1376, hidden_dims=[hidden_dim]*2, output_dim=13, norm='batch', dropout=dropout),\n",
        "    \"Transformer\": lambda hidden_dim, dropout, **kwargs: TransformerMultiModalClassifier(norm='batch', dropout=dropout),\n",
        "    \"ViTLike\": lambda hidden_dim, dropout, **kwargs: ViTLikeClassifier(input_dim=1376, output_dim=13, hidden_dim=hidden_dim, dropout=dropout),\n",
        "    \"FTTransformer\": lambda hidden_dim, dropout, **kwargs: FTTransformerClassifier(input_dim=1376, output_dim=13, hidden_dim=hidden_dim, dropout=dropout),\n",
        "    \"ResMLP\": lambda hidden_dim, dropout: ResidualMLP(input_dim=1376, hidden_dim=hidden_dim, output_dim=13, dropout=dropout),\n",
        "    \"TransformerMLP\": lambda hidden_dim, dropout, d_token, n_layers, n_heads: TransformerMLPClassifier(input_dim=1376, output_dim=13, hidden_dim=hidden_dim, dropout=dropout, d_token=d_token, n_layers=n_layers, n_heads=n_heads),\n",
        "    \"SwinMLP\": lambda hidden_dim, dropout, **kwargs: SwinMLPClassifier(input_dim=1376, output_dim=13, hidden_dim=hidden_dim, dropout=dropout),\n",
        "    \"SAINT\": lambda hidden_dim, dropout, n_heads, **kwargs: SAINTClassifier(input_dim=1376, output_dim=13, hidden_dim=hidden_dim, dropout=dropout, n_heads=n_heads),\n",
        "\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LmbPEy7D0PPS",
        "outputId": "e953b784-3cb4-445c-a716-3a5cae16dbdf"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score, accuracy_score\n",
        "from collections import defaultdict\n",
        "\n",
        "from optuna.pruners import MedianPruner\n",
        "\n",
        "\n",
        "def objective(trial, name, constructor, train_loader, val_loader, test_loader, pathologies, epochs=10):\n",
        "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [256, 512, 1024])\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "\n",
        "    config = {\"hidden_dim\": hidden_dim, \"dropout\": dropout}\n",
        "    if \"TransformerMLP\" in name:\n",
        "        config[\"d_token\"] = trial.suggest_categorical(\"d_token\", [64, 128])\n",
        "        config[\"n_layers\"] = trial.suggest_int(\"n_layers\", 2, 4)\n",
        "        config[\"n_heads\"] = trial.suggest_categorical(\"n_heads\", [2, 4, 8])\n",
        "    elif \"SAINT\" in name:\n",
        "        config[\"n_heads\"] = trial.suggest_categorical(\"n_heads\", [2, 4, 8])\n",
        "\n",
        "    model = constructor(**config)\n",
        "    model_name = f\"{name}_optuna_trial{trial.number}\"\n",
        "\n",
        "    _, auc, _ = train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        test_loader=test_loader,\n",
        "        pathologies=pathologies,\n",
        "        epochs=epochs,\n",
        "        return_metrics=True,\n",
        "        model_name=model_name\n",
        "    )\n",
        "\n",
        "    return np.mean(auc)\n",
        "\n",
        "\n",
        "def run_model_sweep_optuna(model_constructors, train_loader, val_loader, test_loader, pathologies, epochs=10, n_trials=20):\n",
        "    all_results = []\n",
        "    loss_dict, auc_dict = defaultdict(list), defaultdict(list)\n",
        "    best_configs = {}\n",
        "\n",
        "    for idx, (name, constructor) in enumerate(model_constructors.items(), 1):\n",
        "        print(f\"\\nüîç Optimizing {name}...  (Model {idx}/{len(model_constructors)})\")\n",
        "\n",
        "        study = optuna.create_study(\n",
        "            direction=\"maximize\",\n",
        "            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
        "        )\n",
        "        study.optimize(\n",
        "            lambda trial: objective(trial, name, constructor, train_loader, val_loader, test_loader, pathologies, epochs),\n",
        "            n_trials=n_trials\n",
        "        )\n",
        "\n",
        "        best_config = study.best_params\n",
        "        best_configs[name] = best_config\n",
        "\n",
        "        model = constructor(**best_config)\n",
        "        model_name = f\"{name}_optuna\"\n",
        "        loss, auc, df = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            test_loader=test_loader,\n",
        "            pathologies=pathologies,\n",
        "            epochs=epochs,\n",
        "            return_metrics=True,\n",
        "            model_name=model_name\n",
        "        )\n",
        "        loss_dict[model_name] = loss\n",
        "        auc_dict[model_name] = auc\n",
        "        df['model'] = model_name\n",
        "\n",
        "        all_results.append(df.reset_index())\n",
        "\n",
        "    summary_df = pd.concat(all_results, axis=0).rename(columns={\"index\": \"class\"}).reset_index(drop=True)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for name, losses in loss_dict.items():\n",
        "        plt.plot(losses, label=name)\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for name, aucs in auc_dict.items():\n",
        "        plt.plot(aucs, label=name)\n",
        "    plt.title(\"Validation AUC\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    avg_df = summary_df.groupby(\"model\")[[\"AUC\", \"F1\", \"mAP\", \"Accuracy\"]].mean().round(4)\n",
        "    best_model = avg_df[\"AUC\"].idxmax()\n",
        "\n",
        "    print(\"\\nüìä Mean Metrics Per Model:\")\n",
        "    display(avg_df.style.highlight_max(color='lightgreen', axis=0))\n",
        "    print(f\"\\nüèÜ Best Model by AUC: {best_model}\")\n",
        "\n",
        "    for name, cfg in best_configs.items():\n",
        "        print(f\"üîß Best config for {name}: {cfg}\")\n",
        "\n",
        "    return summary_df, loss_dict, auc_dict, best_model\n",
        "\n",
        "\n",
        "summary_df, loss_dict, auc_dict, best_model = run_model_sweep_optuna(\n",
        "    model_constructors, train_loader, val_loader, test_loader, dataset.pathologies\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Sweep Summary:\")\n",
        "display(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs4uZ8wt0XPo",
        "outputId": "7bfed3bd-c984-4513-9dda-bbb52dc0180a"
      },
      "outputs": [],
      "source": [
        "! pip install optuna"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qRWzC4aBubbO",
        "qwRzChTJEVuK",
        "wvam-dmSEbYF",
        "Qv5Bh2VsY8NW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llmtrain",
      "language": "python",
      "name": "llmtrain"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
