{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values:\n",
      " [31.94009062 57.33811013 47.69681726 41.81932982 22.30756475 22.30650152\n",
      " 17.99054428 53.61162967 41.92761467 46.64235269 16.33756616 58.1842585\n",
      " 52.12464147 24.79019662 23.44511934 23.51474629 28.84132837 38.56169382\n",
      " 34.47052464 28.2677049  42.40094542 21.57914496 28.30806092 31.57958853\n",
      " 35.53396407 50.04110481 24.23190319 38.09787959 41.5440952  17.47774758\n",
      " 42.21104494 22.94697303 18.29769604 57.25749716 57.99569029 51.06471407\n",
      " 28.85770542 19.7356234  45.59149445 34.83231387 20.80969306 37.2578151\n",
      " 16.94605392 55.51344806 26.83733127 44.63447509 29.17055792 38.35502656\n",
      " 39.52942939 23.57866055 58.16992246 49.59839873 56.84373176 54.87458777\n",
      " 41.78589456 56.06682671 19.33098194 24.06920578 17.42383172 29.77090085\n",
      " 32.56326353 27.3913807  51.96131759 31.15604076 27.81391292 39.3524818\n",
      " 21.64219607 50.79139906 18.7164185  58.93261608 49.47109199 24.18966968\n",
      " 15.67360974 51.37610193 46.58878462 47.56515892 49.428139   18.69411415\n",
      " 31.23152421 20.53775301 53.47618278 42.90545644 30.01632733 18.23187322\n",
      " 29.1384341  29.76442064 47.59156357 43.53401481 54.5389324  36.24564039\n",
      " 20.70196092 46.87034603 48.96594231 40.17154575 49.41477528 37.19692616\n",
      " 38.47249252 34.27639431 16.55067895 20.18609535]\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "         Metric     Value\n",
      "0           MAE  1.402085\n",
      "1           MSE  3.226338\n",
      "2          RMSE  1.796201\n",
      "3     R2 Square  0.981401\n",
      "4  Adj R Square  0.981211\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.beta = None\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self._is_fit = False\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        return np.c_[np.ones(X.shape[0]), X] if self.fit_intercept else X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"The number of rows in X and y must match.\")\n",
    "\n",
    "        X = self._add_intercept(X)\n",
    "        self.beta = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "        self._is_fit = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self._is_fit:\n",
    "            raise RuntimeError(\"You must call `fit` before `predict`.\")\n",
    "        X = self._add_intercept(X)\n",
    "        return X @ self.beta\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        n, p = X.shape[0], X.shape[1] if self.fit_intercept else X.shape[1] - 1\n",
    "\n",
    "        mae = np.mean(np.abs(y - y_pred))\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2_square = 1 - (np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
    "        adj_rsquared = 1 - (1 - r2_square) * (n - 1) / (n - p - 1)\n",
    "\n",
    "        metrics = pd.DataFrame({\n",
    "            \"Metric\": [\"MAE\", \"MSE\", \"RMSE\", \"R2 Square\", \"Adj R Square\"],\n",
    "            \"Value\": [mae, mse, rmse, r2_square, adj_rsquared]\n",
    "        })\n",
    "        return metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    data_size = 100\n",
    "    X = np.random.uniform(low=1.0, high=10.0, size=(data_size, 1))\n",
    "    y = 5 * X[:, 0] + 10 + np.random.normal(0, 2, size=data_size)\n",
    "\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    print(\"Predicted Values:\\n\", y_pred)\n",
    "\n",
    "    metrics = model.evaluate(X, y)\n",
    "    print(\"\\nModel Evaluation Metrics:\")\n",
    "    print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics Comparison:\n",
      "   Custom Model            Sklearn Model           \n",
      "         Metric      Value        Metric      Value\n",
      "0           MAE   6.523364           MAE   6.523364\n",
      "1           MSE  78.319574           MSE  78.319574\n",
      "2          RMSE   8.849835          RMSE   8.849835\n",
      "3     R2 Square   0.991358     R2 Square   0.991358\n",
      "4  Adj R Square   0.991131        Adj R2   0.991131\n",
      "\n",
      "Test Set Metrics Comparison:\n",
      "   Custom Model             Sklearn Model            \n",
      "         Metric       Value        Metric       Value\n",
      "0           MAE    9.448952           MAE    9.448952\n",
      "1           MSE  125.890300           MSE  125.890300\n",
      "2          RMSE   11.220085          RMSE   11.220085\n",
      "3     R2 Square    0.983350     R2 Square    0.983350\n",
      "4  Adj R Square    0.982911        Adj R2    0.982911\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "y = y.reshape(-1, 1) \n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "custom_model = LinearRegression()\n",
    "custom_model.fit(X_train, y_train)  \n",
    "custom_val_metrics = custom_model.evaluate(X_val, y_val)  \n",
    "custom_test_metrics = custom_model.evaluate(X_test, y_test)  \n",
    "\n",
    "sklearn_model = SklearnLinearRegression()\n",
    "sklearn_model.fit(X_train, y_train) \n",
    "y_val_pred_sklearn = sklearn_model.predict(X_val)\n",
    "y_test_pred_sklearn = sklearn_model.predict(X_test)\n",
    "\n",
    "mae_val_sklearn = mean_absolute_error(y_val, y_val_pred_sklearn)\n",
    "mse_val_sklearn = mean_squared_error(y_val, y_val_pred_sklearn)\n",
    "rmse_val_sklearn = np.sqrt(mse_val_sklearn)\n",
    "r2_val_sklearn = r2_score(y_val, y_val_pred_sklearn)\n",
    "n, p = X_val.shape[0], X_val.shape[1]\n",
    "adj_r2_val_sklearn = 1 - (1 - r2_val_sklearn) * (n - 1) / (n - p - 1)\n",
    "\n",
    "mae_test_sklearn = mean_absolute_error(y_test, y_test_pred_sklearn)\n",
    "mse_test_sklearn = mean_squared_error(y_test, y_test_pred_sklearn)\n",
    "rmse_test_sklearn = np.sqrt(mse_test_sklearn)\n",
    "r2_test_sklearn = r2_score(y_test, y_test_pred_sklearn)\n",
    "n, p = X_test.shape[0], X_test.shape[1]\n",
    "adj_r2_test_sklearn = 1 - (1 - r2_test_sklearn) * (n - 1) / (n - p - 1)\n",
    "\n",
    "val_metrics_sklearn = pd.DataFrame({\n",
    "    \"Metric\": [\"MAE\", \"MSE\", \"RMSE\", \"R2 Square\", \"Adj R2\"],\n",
    "    \"Value\": [mae_val_sklearn, mse_val_sklearn, rmse_val_sklearn, r2_val_sklearn, adj_r2_val_sklearn]\n",
    "})\n",
    "\n",
    "test_metrics_sklearn = pd.DataFrame({\n",
    "    \"Metric\": [\"MAE\", \"MSE\", \"RMSE\", \"R2 Square\", \"Adj R2\"],\n",
    "    \"Value\": [mae_test_sklearn, mse_test_sklearn, rmse_test_sklearn, r2_test_sklearn, adj_r2_test_sklearn]\n",
    "})\n",
    "\n",
    "print(\"Validation Set Metrics Comparison:\")\n",
    "print(pd.concat([custom_val_metrics, val_metrics_sklearn], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "print(\"\\nTest Set Metrics Comparison:\")\n",
    "print(pd.concat([custom_test_metrics, test_metrics_sklearn], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics Comparison:\n",
      "  Custom Model           Sklearn Model          \n",
      "        Metric     Value        Metric     Value\n",
      "0     Accuracy  0.983333      Accuracy  0.993333\n",
      "1    Precision  0.967949     Precision  0.993377\n",
      "2       Recall  1.000000        Recall  0.993377\n",
      "3     F1 Score  0.983713      F1 Score  0.993377\n",
      "4     Log Loss  0.369571      Log Loss  0.054250\n",
      "\n",
      "Validation Metrics Comparison:\n",
      "  Custom Model           Sklearn Model          \n",
      "        Metric     Value        Metric     Value\n",
      "0     Accuracy  0.970000      Accuracy  0.990000\n",
      "1    Precision  0.940000     Precision  0.979167\n",
      "2       Recall  1.000000        Recall  1.000000\n",
      "3     F1 Score  0.969072      F1 Score  0.989474\n",
      "4     Log Loss  0.329722      Log Loss  0.040553\n",
      "\n",
      "Test Metrics Comparison:\n",
      "  Custom Model           Sklearn Model          \n",
      "        Metric     Value        Metric     Value\n",
      "0     Accuracy  0.960000      Accuracy  1.000000\n",
      "1    Precision  0.920000     Precision  1.000000\n",
      "2       Recall  1.000000        Recall  1.000000\n",
      "3     F1 Score  0.958333      F1 Score  1.000000\n",
      "4     Log Loss  0.364836      Log Loss  0.050895\n",
      "\n",
      "Cross-Validation Metrics Comparison:\n",
      "           Custom Model  Sklearn Model\n",
      "Metric                                \n",
      "Accuracy       0.970000       0.990000\n",
      "Precision      0.979524       0.987389\n",
      "Recall         0.960568       0.993939\n",
      "F1 Score       0.969318       0.990516\n",
      "Log Loss       0.361900       0.059947\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True):\n",
    "        assert penalty in [\"l2\", \"l1\"], \"penalty must be 'l1' or 'l2'\"\n",
    "        self.beta = None\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        return np.c_[np.ones(X.shape[0]), X] if self.fit_intercept else X\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-6, max_iter=1000):\n",
    "        X = self._add_intercept(X)\n",
    "        self.beta = np.random.randn(X.shape[1])\n",
    "        prev_loss = float(\"inf\")\n",
    "\n",
    "        for _ in range(int(max_iter)):\n",
    "            y_pred = _sigmoid(X @ self.beta)\n",
    "            loss = self._compute_loss(y, y_pred)\n",
    "            if abs(prev_loss - loss) < tol:\n",
    "                break\n",
    "            prev_loss = loss\n",
    "            grad = self._compute_gradient(X, y, y_pred)\n",
    "            self.beta -= lr * grad\n",
    "\n",
    "    def _compute_loss(self, y, y_pred):\n",
    "        N = len(y)\n",
    "        if self.penalty == \"l2\":\n",
    "            penalty = (self.gamma / 2) * np.sum(self.beta ** 2)\n",
    "        elif self.penalty == \"l1\":\n",
    "            penalty = self.gamma * np.sum(np.abs(self.beta))\n",
    "        else:\n",
    "            penalty = 0\n",
    "        return -(np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) / N) + penalty\n",
    "\n",
    "    def _compute_gradient(self, X, y, y_pred):\n",
    "        N = len(y)\n",
    "        if self.penalty == \"l2\":\n",
    "            penalty_grad = self.gamma * self.beta\n",
    "        elif self.penalty == \"l1\":\n",
    "            penalty_grad = self.gamma * np.sign(self.beta)\n",
    "        else:\n",
    "            penalty_grad = 0\n",
    "        return -(X.T @ (y - y_pred)) / N + penalty_grad\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self._add_intercept(X)\n",
    "        return _sigmoid(X @ self.beta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y, y_pred),\n",
    "        \"Precision\": precision_score(y, y_pred),\n",
    "        \"Recall\": recall_score(y, y_pred),\n",
    "        \"F1 Score\": f1_score(y, y_pred),\n",
    "        \"Log Loss\": log_loss(y, y_proba)\n",
    "    }\n",
    "    return pd.DataFrame(metrics.items(), columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "def cross_validate(model, X, y, cv=\"kfold\", n_splits=5):\n",
    "    if cv == \"kfold\":\n",
    "        cv_splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"stratified\":\n",
    "        cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"loo\":\n",
    "        cv_splitter = LeaveOneOut()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cv type. Choose from 'kfold', 'stratified', 'loo'.\")\n",
    "\n",
    "    metrics_list = []\n",
    "    for train_idx, val_idx in cv_splitter.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_val, y_val)\n",
    "        metrics_list.append(metrics.set_index(\"Metric\")[\"Value\"])\n",
    "\n",
    "    avg_metrics = pd.DataFrame(metrics_list).mean(axis=0)\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(500, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    custom_model = LogisticRegression(penalty=\"l2\", gamma=0.1)\n",
    "    custom_model.fit(X_train, y_train)\n",
    "    train_metrics_custom = evaluate_model(custom_model, X_train, y_train)\n",
    "    val_metrics_custom = evaluate_model(custom_model, X_val, y_val)\n",
    "    test_metrics_custom = evaluate_model(custom_model, X_test, y_test)\n",
    "\n",
    "    sklearn_model = SklearnLogisticRegression(penalty=\"l2\", C=1/0.1, solver=\"liblinear\")\n",
    "    sklearn_model.fit(X_train, y_train)\n",
    "    train_metrics_sklearn = evaluate_model(sklearn_model, X_train, y_train)\n",
    "    val_metrics_sklearn = evaluate_model(sklearn_model, X_val, y_val)\n",
    "    test_metrics_sklearn = evaluate_model(sklearn_model, X_test, y_test)\n",
    "\n",
    "    print(\"Train Metrics Comparison:\")\n",
    "    print(pd.concat([train_metrics_custom, train_metrics_sklearn], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nValidation Metrics Comparison:\")\n",
    "    print(pd.concat([val_metrics_custom, val_metrics_sklearn], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nTest Metrics Comparison:\")\n",
    "    print(pd.concat([test_metrics_custom, test_metrics_sklearn], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nCross-Validation Metrics Comparison:\")\n",
    "    custom_cv_metrics = cross_validate(custom_model, X_train, y_train, cv=\"kfold\", n_splits=5)\n",
    "    sklearn_cv_metrics = cross_validate(sklearn_model, X_train, y_train, cv=\"kfold\", n_splits=5)\n",
    "    print(pd.concat([custom_cv_metrics, sklearn_cv_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassProbEstimator:\n",
    "    def fit(self, X, y):\n",
    "        self.class_prob = y.sum() / len(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.empty(X.shape[0], dtype=np.float64)\n",
    "        pred.fill(self.class_prob)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class MeanBaseEstimator:\n",
    "    def fit(self, X, y):\n",
    "        self.avg = np.mean(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.empty(X.shape[0], dtype=np.float64)\n",
    "        pred.fill(self.avg)\n",
    "        return pred\n",
    "\n",
    "class MSELoss:\n",
    "    def __call__(self, y, y_pred):\n",
    "        return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "    def base_estimator(self):\n",
    "        return MeanBaseEstimator()\n",
    "\n",
    "    def grad(self, y, y_pred):\n",
    "        return -2 / len(y) * (y - y_pred)\n",
    "\n",
    "    def line_search(self, y, y_pred, h_pred):\n",
    "        # TODO: revise this\n",
    "        Lp = np.sum((y - y_pred) * h_pred)\n",
    "        Lpp = np.sum(h_pred * h_pred)\n",
    "\n",
    "        return 1 if np.sum(Lpp) == 0 else Lp / Lpp\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __call__(self, y, y_pred):\n",
    "        eps = np.finfo(float).eps\n",
    "        return -np.sum(y * np.log(y_pred + eps))\n",
    "\n",
    "    def base_estimator(self):\n",
    "        return ClassProbEstimator()\n",
    "\n",
    "    def grad(self, y, y_pred):\n",
    "        eps = np.finfo(float).eps\n",
    "        return -y * 1 / (y_pred + eps)\n",
    "\n",
    "    def line_search(self, y, y_pred, h_pred):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        self.value = value \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, classifier=True, max_depth=None, n_feats=None, criterion=\"entropy\", seed=None):\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "        self.n_feats = n_feats\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "        self.depth = 0\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        if not classifier and criterion not in [\"mse\"]:\n",
    "            raise ValueError(\"`mse` is the only valid criterion for regression.\")\n",
    "        if classifier and criterion not in [\"entropy\", \"gini\"]:\n",
    "            raise ValueError(\"Valid criteria for classification are 'entropy' and 'gini'.\")\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None\n",
    "        self.n_feats = X.shape[1] if self.n_feats is None else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def _grow(self, X, Y, cur_depth=0):\n",
    "        if len(set(Y)) == 1:\n",
    "            return Leaf(self._leaf_value(Y))\n",
    "\n",
    "        if cur_depth >= self.max_depth:\n",
    "            return Leaf(self._leaf_value(Y))\n",
    "\n",
    "        cur_depth += 1\n",
    "        self.depth = max(self.depth, cur_depth)\n",
    "\n",
    "        feat_idxs = np.random.choice(X.shape[1], self.n_feats, replace=False)\n",
    "\n",
    "        feat, thresh = self._segment(X, Y, feat_idxs)\n",
    "        l_idx = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r_idx = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "\n",
    "        left = self._grow(X[l_idx], Y[l_idx], cur_depth)\n",
    "        right = self._grow(X[r_idx], Y[r_idx], cur_depth)\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    def _segment(self, X, Y, feat_idxs):\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "\n",
    "        for i in feat_idxs:\n",
    "            thresholds = np.unique(X[:, i])\n",
    "            for thresh in thresholds:\n",
    "                gain = self._impurity_gain(Y, X[:, i], thresh)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = i\n",
    "                    split_thresh = thresh\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _impurity_gain(self, Y, feat_values, thresh):\n",
    "        if self.criterion == \"entropy\":\n",
    "            impurity = entropy\n",
    "        elif self.criterion == \"gini\":\n",
    "            impurity = gini\n",
    "        elif self.criterion == \"mse\":\n",
    "            impurity = mse\n",
    "\n",
    "        parent_loss = impurity(Y)\n",
    "        left_idx = feat_values <= thresh\n",
    "        right_idx = feat_values > thresh\n",
    "\n",
    "        if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:\n",
    "            return 0\n",
    "\n",
    "        left_loss = impurity(Y[left_idx])\n",
    "        right_loss = impurity(Y[right_idx])\n",
    "        n = len(Y)\n",
    "        n_l, n_r = np.sum(left_idx), np.sum(right_idx)\n",
    "\n",
    "        return parent_loss - (n_l / n) * left_loss - (n_r / n) * right_loss\n",
    "\n",
    "    def _leaf_value(self, Y):\n",
    "        if self.classifier:\n",
    "            probs = np.bincount(Y, minlength=self.n_classes) / len(Y)\n",
    "            return probs\n",
    "        return np.mean(Y)\n",
    "\n",
    "    def _traverse(self, x, node):\n",
    "        if isinstance(node, Leaf):\n",
    "            if self.classifier:\n",
    "                return node.value.argmax()\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse(x, node.left)\n",
    "        return self._traverse(x, node.right)\n",
    "\n",
    "\n",
    "def entropy(Y):\n",
    "    ps = np.bincount(Y) / len(Y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "def gini(Y):\n",
    "    ps = np.bincount(Y) / len(Y)\n",
    "    return 1 - np.sum(ps ** 2)\n",
    "\n",
    "def mse(Y):\n",
    "    return np.mean((Y - np.mean(Y)) ** 2)\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, classifier=True):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    if classifier:  \n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y, y_pred),\n",
    "            \"Precision\": precision_score(y, y_pred, average=\"weighted\"),\n",
    "            \"Recall\": recall_score(y, y_pred, average=\"weighted\"),\n",
    "            \"F1 Score\": f1_score(y, y_pred, average=\"weighted\"),\n",
    "        }\n",
    "    else:  \n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y) - 1) / (len(y) - X.shape[1] - 1)\n",
    "        metrics = {\n",
    "            \"MAE\": mean_absolute_error(y, y_pred),\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": np.sqrt(mse),\n",
    "            \"R2 Square\": r2,\n",
    "            \"Adj R Square\": adj_r2,\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(metrics.items(), columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "def cross_validate(model, X, y, cv=\"kfold\", n_splits=5, classifier=True):\n",
    "    if cv == \"kfold\":\n",
    "        cv_splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"stratified\":\n",
    "        if not classifier:\n",
    "            raise ValueError(\"Stratified K-Fold 仅适用于分类问题。\")\n",
    "        cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"loo\":\n",
    "        cv_splitter = LeaveOneOut()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cv type. Choose from 'kfold', 'stratified', 'loo'.\")\n",
    "\n",
    "    metrics_list = []\n",
    "    for train_idx, val_idx in cv_splitter.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_val, y_val, classifier=classifier)\n",
    "        metrics_list.append(metrics.set_index(\"Metric\")[\"Value\"])\n",
    "\n",
    "    avg_metrics = pd.DataFrame(metrics_list).mean(axis=0)\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics Comparison:\n",
      "   Custom Model           Sklearn Model          \n",
      "         Metric     Value        Metric     Value\n",
      "0           MAE  1.110582           MAE  1.110582\n",
      "1           MSE  2.496919           MSE  2.496919\n",
      "2          RMSE  1.580164          RMSE  1.580164\n",
      "3     R2 Square  0.979766     R2 Square  0.979766\n",
      "4  Adj R Square  0.979420  Adj R Square  0.979420\n",
      "\n",
      "Validation Metrics Comparison:\n",
      "   Custom Model           Sklearn Model          \n",
      "         Metric     Value        Metric     Value\n",
      "0           MAE  2.302911           MAE  2.302911\n",
      "1           MSE  9.020365           MSE  9.020365\n",
      "2          RMSE  3.003392          RMSE  3.003392\n",
      "3     R2 Square  0.925768     R2 Square  0.925768\n",
      "4  Adj R Square  0.921756  Adj R Square  0.921756\n",
      "\n",
      "Test Metrics Comparison:\n",
      "   Custom Model            Sklearn Model           \n",
      "         Metric      Value        Metric      Value\n",
      "0           MAE   2.541860           MAE   2.541860\n",
      "1           MSE  10.888125           MSE  10.888125\n",
      "2          RMSE   3.299716          RMSE   3.299716\n",
      "3     R2 Square   0.890770     R2 Square   0.890770\n",
      "4  Adj R Square   0.884866  Adj R Square   0.884866\n",
      "\n",
      "Cross-Validation Metrics Comparison:\n",
      "              Custom Model  Sklearn Model\n",
      "Metric                                   \n",
      "MAE               2.147929       2.147929\n",
      "MSE               7.337608       7.337608\n",
      "RMSE              2.691263       2.691263\n",
      "R2 Square         0.937511       0.937511\n",
      "Adj R Square      0.934134       0.934134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(200, 2) * 10\n",
    "y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(200)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "custom_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "custom_model.fit(X_train, y_train)\n",
    "\n",
    "sklearn_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "custom_train_metrics = evaluate_model(custom_model, X_train, y_train, classifier=False)\n",
    "custom_val_metrics = evaluate_model(custom_model, X_val, y_val, classifier=False)\n",
    "custom_test_metrics = evaluate_model(custom_model, X_test, y_test, classifier=False)\n",
    "\n",
    "sklearn_train_metrics = evaluate_model(sklearn_model, X_train, y_train, classifier=False)\n",
    "sklearn_val_metrics = evaluate_model(sklearn_model, X_val, y_val, classifier=False)\n",
    "sklearn_test_metrics = evaluate_model(sklearn_model, X_test, y_test, classifier=False)\n",
    "\n",
    "print(\"Train Metrics Comparison:\")\n",
    "print(pd.concat([custom_train_metrics, sklearn_train_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "print(\"\\nValidation Metrics Comparison:\")\n",
    "print(pd.concat([custom_val_metrics, sklearn_val_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "print(\"\\nTest Metrics Comparison:\")\n",
    "print(pd.concat([custom_test_metrics, sklearn_test_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "custom_cv_metrics = cross_validate(custom_model, X, y, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "sklearn_cv_metrics = cross_validate(sklearn_model, X, y, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "\n",
    "print(\"\\nCross-Validation Metrics Comparison:\")\n",
    "print(pd.concat([custom_cv_metrics, sklearn_cv_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics Comparison:\n",
      "  Custom Model       Sklearn Model          \n",
      "        Metric Value        Metric     Value\n",
      "0     Accuracy   1.0      Accuracy  0.991667\n",
      "1    Precision   1.0     Precision  0.991821\n",
      "2       Recall   1.0        Recall  0.991667\n",
      "3     F1 Score   1.0      F1 Score  0.991674\n",
      "\n",
      "Validation Metrics Comparison:\n",
      "  Custom Model       Sklearn Model          \n",
      "        Metric Value        Metric     Value\n",
      "0     Accuracy   0.9      Accuracy  0.925000\n",
      "1    Precision   0.9     Precision  0.925333\n",
      "2       Recall   0.9        Recall  0.925000\n",
      "3     F1 Score   0.9      F1 Score  0.924556\n",
      "\n",
      "Test Metrics Comparison:\n",
      "  Custom Model           Sklearn Model          \n",
      "        Metric     Value        Metric     Value\n",
      "0     Accuracy  0.975000      Accuracy  0.975000\n",
      "1    Precision  0.976389     Precision  0.976389\n",
      "2       Recall  0.975000        Recall  0.975000\n",
      "3     F1 Score  0.975079      F1 Score  0.975079\n",
      "\n",
      "Cross-Validation Metrics Comparison:\n",
      "           Custom Model  Sklearn Model\n",
      "Metric                                \n",
      "Accuracy       0.935000       0.945000\n",
      "Precision      0.937763       0.947839\n",
      "Recall         0.935000       0.945000\n",
      "F1 Score       0.934872       0.944890\n",
      "\n",
      "Regression Train Metrics Comparison:\n",
      "   Custom Model           Sklearn Model          \n",
      "         Metric     Value        Metric     Value\n",
      "0           MAE  0.974338           MAE  0.883184\n",
      "1           MSE  1.589639           MSE  1.375499\n",
      "2          RMSE  1.260809          RMSE  1.172817\n",
      "3     R2 Square  0.985552     R2 Square  0.987498\n",
      "4  Adj R Square  0.985305  Adj R Square  0.987284\n",
      "\n",
      "Regression Validation Metrics Comparison:\n",
      "   Custom Model           Sklearn Model          \n",
      "         Metric     Value        Metric     Value\n",
      "0           MAE  2.082230           MAE  1.755790\n",
      "1           MSE  6.936954           MSE  4.999284\n",
      "2          RMSE  2.633810          RMSE  2.235908\n",
      "3     R2 Square  0.921737     R2 Square  0.943598\n",
      "4  Adj R Square  0.917507  Adj R Square  0.940549\n",
      "\n",
      "Regression Test Metrics Comparison:\n",
      "   Custom Model           Sklearn Model          \n",
      "         Metric     Value        Metric     Value\n",
      "0           MAE  1.667453           MAE  1.984060\n",
      "1           MSE  4.121333           MSE  6.203669\n",
      "2          RMSE  2.030107          RMSE  2.490717\n",
      "3     R2 Square  0.969024     R2 Square  0.953372\n",
      "4  Adj R Square  0.967349  Adj R Square  0.950852\n",
      "\n",
      "Regression Cross-Validation Metrics Comparison:\n",
      "              Custom Model  Sklearn Model\n",
      "Metric                                   \n",
      "MAE               1.664078       1.479470\n",
      "MSE               4.518450       3.462093\n",
      "RMSE              2.118384       1.859338\n",
      "R2 Square         0.956651       0.967061\n",
      "Adj R Square      0.954308       0.965280\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, LeaveOneOut\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def bootstrap_sample(X, Y):\n",
    "    N, M = X.shape\n",
    "    idxs = np.random.choice(N, N, replace=True)\n",
    "    return X[idxs], Y[idxs]\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees, max_depth, n_feats, classifier=True, criterion=\"entropy\"):\n",
    "        self.trees = []\n",
    "        self.n_trees = n_trees\n",
    "        self.n_feats = n_feats\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            X_samp, Y_samp = bootstrap_sample(X, Y)\n",
    "            tree = DecisionTree(\n",
    "                n_feats=self.n_feats,\n",
    "                max_depth=self.max_depth,\n",
    "                criterion=self.criterion,\n",
    "                classifier=self.classifier,\n",
    "            )\n",
    "            tree.fit(X_samp, Y_samp)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([[t._traverse(x, t.root) for x in X] for t in self.trees])\n",
    "        return self._vote(tree_preds)\n",
    "\n",
    "    def _vote(self, predictions):\n",
    "        if self.classifier:\n",
    "            out = [np.bincount(x).argmax() for x in predictions.T]\n",
    "        else:\n",
    "            out = [np.mean(x) for x in predictions.T]\n",
    "        return np.array(out)\n",
    "\n",
    "def evaluate_model(model, X, y, classifier=True):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    if classifier:  \n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y, y_pred),\n",
    "            \"Precision\": precision_score(y, y_pred, average=\"weighted\"),\n",
    "            \"Recall\": recall_score(y, y_pred, average=\"weighted\"),\n",
    "            \"F1 Score\": f1_score(y, y_pred, average=\"weighted\"),\n",
    "        }\n",
    "    else:  \n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y) - 1) / (len(y) - X.shape[1] - 1)\n",
    "        metrics = {\n",
    "            \"MAE\": mean_absolute_error(y, y_pred),\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": np.sqrt(mse),\n",
    "            \"R2 Square\": r2,\n",
    "            \"Adj R Square\": adj_r2,\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(metrics.items(), columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "def cross_validate(model, X, y, cv=\"kfold\", n_splits=5, classifier=True):\n",
    "    if cv == \"kfold\":\n",
    "        cv_splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"stratified\":\n",
    "        if not classifier:\n",
    "            raise ValueError(\"Stratified K-Fold only works for classification tasks.\")\n",
    "        cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"loo\":\n",
    "        cv_splitter = LeaveOneOut()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cv type. Choose from 'kfold', 'stratified', 'loo'.\")\n",
    "\n",
    "    metrics_list = []\n",
    "    for train_idx, val_idx in cv_splitter.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_val, y_val, classifier=classifier)\n",
    "        metrics_list.append(metrics.set_index(\"Metric\")[\"Value\"])\n",
    "\n",
    "    avg_metrics = pd.DataFrame(metrics_list).mean(axis=0)\n",
    "    return avg_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    X_class = np.random.randn(200, 2)\n",
    "    y_class = (X_class[:, 0] + X_class[:, 1] > 0).astype(int)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_class, y_class, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    custom_rf = RandomForest(n_trees=10, max_depth=5, n_feats=2, classifier=True, criterion=\"gini\")\n",
    "    custom_rf.fit(X_train, y_train)\n",
    "\n",
    "    sklearn_rf = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
    "    sklearn_rf.fit(X_train, y_train)\n",
    "\n",
    "    custom_train_metrics = evaluate_model(custom_rf, X_train, y_train, classifier=True)\n",
    "    custom_val_metrics = evaluate_model(custom_rf, X_val, y_val, classifier=True)\n",
    "    custom_test_metrics = evaluate_model(custom_rf, X_test, y_test, classifier=True)\n",
    "\n",
    "    sklearn_train_metrics = evaluate_model(sklearn_rf, X_train, y_train, classifier=True)\n",
    "    sklearn_val_metrics = evaluate_model(sklearn_rf, X_val, y_val, classifier=True)\n",
    "    sklearn_test_metrics = evaluate_model(sklearn_rf, X_test, y_test, classifier=True)\n",
    "\n",
    "    print(\"Train Metrics Comparison:\")\n",
    "    print(pd.concat([custom_train_metrics, sklearn_train_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nValidation Metrics Comparison:\")\n",
    "    print(pd.concat([custom_val_metrics, sklearn_val_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nTest Metrics Comparison:\")\n",
    "    print(pd.concat([custom_test_metrics, sklearn_test_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    custom_cv_metrics = cross_validate(custom_rf, X_class, y_class, cv=\"stratified\", n_splits=5, classifier=True)\n",
    "    sklearn_cv_metrics = cross_validate(sklearn_rf, X_class, y_class, cv=\"stratified\", n_splits=5, classifier=True)\n",
    "\n",
    "    print(\"\\nCross-Validation Metrics Comparison:\")\n",
    "    print(pd.concat([custom_cv_metrics, sklearn_cv_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    X_reg = np.random.rand(200, 2) * 10\n",
    "    y_reg = 3 * X_reg[:, 0] + 2 * X_reg[:, 1] + np.random.randn(200)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_reg, y_reg, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    custom_rf = RandomForest(n_trees=10, max_depth=5, n_feats=2, classifier=False, criterion=\"mse\")\n",
    "    custom_rf.fit(X_train, y_train)\n",
    "\n",
    "    sklearn_rf = RandomForestRegressor(n_estimators=10, max_depth=5, random_state=42)\n",
    "    sklearn_rf.fit(X_train, y_train)\n",
    "\n",
    "    custom_train_metrics = evaluate_model(custom_rf, X_train, y_train, classifier=False)\n",
    "    custom_val_metrics = evaluate_model(custom_rf, X_val, y_val, classifier=False)\n",
    "    custom_test_metrics = evaluate_model(custom_rf, X_test, y_test, classifier=False)\n",
    "\n",
    "    sklearn_train_metrics = evaluate_model(sklearn_rf, X_train, y_train, classifier=False)\n",
    "    sklearn_val_metrics = evaluate_model(sklearn_rf, X_val, y_val, classifier=False)\n",
    "    sklearn_test_metrics = evaluate_model(sklearn_rf, X_test, y_test, classifier=False)\n",
    "\n",
    "    print(\"\\nRegression Train Metrics Comparison:\")\n",
    "    print(pd.concat([custom_train_metrics, sklearn_train_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nRegression Validation Metrics Comparison:\")\n",
    "    print(pd.concat([custom_val_metrics, sklearn_val_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nRegression Test Metrics Comparison:\")\n",
    "    print(pd.concat([custom_test_metrics, sklearn_test_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    custom_cv_metrics = cross_validate(custom_rf, X_reg, y_reg, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "    sklearn_cv_metrics = cross_validate(sklearn_rf, X_reg, y_reg, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "\n",
    "    print(\"\\nRegression Cross-Validation Metrics Comparison:\")\n",
    "    print(pd.concat([custom_cv_metrics, sklearn_cv_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Train Metrics Comparison:\n",
      "  Custom Model           Sklearn Model          \n",
      "        Metric     Value        Metric     Value\n",
      "0     Accuracy  0.983333      Accuracy  0.991667\n",
      "1    Precision  0.983816     Precision  0.991789\n",
      "2       Recall  0.983333        Recall  0.991667\n",
      "3     F1 Score  0.983296      F1 Score  0.991658\n",
      "\n",
      "Classification Validation Metrics Comparison:\n",
      "  Custom Model       Sklearn Model          \n",
      "        Metric Value        Metric     Value\n",
      "0     Accuracy   0.9      Accuracy  0.900000\n",
      "1    Precision   0.9     Precision  0.902198\n",
      "2       Recall   0.9        Recall  0.900000\n",
      "3     F1 Score   0.9      F1 Score  0.898667\n",
      "\n",
      "Classification Test Metrics Comparison:\n",
      "  Custom Model           Sklearn Model      \n",
      "        Metric     Value        Metric Value\n",
      "0     Accuracy  0.975000      Accuracy   1.0\n",
      "1    Precision  0.976389     Precision   1.0\n",
      "2       Recall  0.975000        Recall   1.0\n",
      "3     F1 Score  0.975079      F1 Score   1.0\n",
      "\n",
      "Classification Cross-Validation Metrics Comparison:\n",
      "           Custom Model  Sklearn Model\n",
      "Metric                                \n",
      "Accuracy       0.925000       0.945000\n",
      "Precision      0.926072       0.946623\n",
      "Recall         0.925000       0.945000\n",
      "F1 Score       0.924953       0.944966\n",
      "\n",
      "Regression Train Metrics Comparison:\n",
      "   Custom Model             Sklearn Model           \n",
      "         Metric       Value        Metric      Value\n",
      "0           MAE    8.501311           MAE   3.486531\n",
      "1           MSE  108.245830           MSE  18.639189\n",
      "2          RMSE   10.404126          RMSE   4.317313\n",
      "3     R2 Square    0.026765     R2 Square   0.832416\n",
      "4  Adj R Square    0.010129  Adj R Square   0.829551\n",
      "\n",
      "Regression Validation Metrics Comparison:\n",
      "   Custom Model            Sklearn Model           \n",
      "         Metric      Value        Metric      Value\n",
      "0           MAE   7.237066           MAE   3.439480\n",
      "1           MSE  85.785692           MSE  18.740596\n",
      "2          RMSE   9.262057          RMSE   4.329041\n",
      "3     R2 Square   0.015399     R2 Square   0.784906\n",
      "4  Adj R Square  -0.037822  Adj R Square   0.773279\n",
      "\n",
      "Regression Test Metrics Comparison:\n",
      "   Custom Model             Sklearn Model           \n",
      "         Metric       Value        Metric      Value\n",
      "0           MAE    9.951292           MAE   4.539570\n",
      "1           MSE  126.132339           MSE  26.691589\n",
      "2          RMSE   11.230865          RMSE   5.166390\n",
      "3     R2 Square   -0.008462     R2 Square   0.786594\n",
      "4  Adj R Square   -0.062974  Adj R Square   0.775058\n",
      "\n",
      "Regression Cross-Validation Metrics Comparison:\n",
      "              Custom Model  Sklearn Model\n",
      "Metric                                   \n",
      "MAE               8.530498       3.844129\n",
      "MSE             108.147055      22.625429\n",
      "RMSE             10.395035       4.753354\n",
      "R2 Square         0.007874       0.792208\n",
      "Adj R Square     -0.045755       0.780976\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, LeaveOneOut\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def to_one_hot(labels, n_classes=None):\n",
    "    if labels.ndim > 1:\n",
    "        raise ValueError(\"labels must have dimension 1, but got {}\".format(labels.ndim))\n",
    "\n",
    "    N = labels.size\n",
    "    n_cols = np.max(labels) + 1 if n_classes is None else n_classes\n",
    "    one_hot = np.zeros((N, n_cols))\n",
    "    one_hot[np.arange(N), labels] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "class GradientBoostedDecisionTree:\n",
    "    def __init__(self, n_iter, max_depth=None, classifier=True, learning_rate=1, loss=\"crossentropy\", step_size=\"constant\"):\n",
    "        self.loss = loss\n",
    "        self.weights = None\n",
    "        self.learners = None\n",
    "        self.out_dims = None\n",
    "        self.n_iter = n_iter\n",
    "        self.max_depth = max_depth\n",
    "        self.step_size = step_size\n",
    "        self.classifier = classifier\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        if self.loss == \"mse\":\n",
    "            loss = MSELoss()\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            loss = CrossEntropyLoss()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function: {}\".format(self.loss))\n",
    "\n",
    "        if self.classifier:\n",
    "            Y = to_one_hot(Y.flatten())\n",
    "        else:\n",
    "            Y = Y.reshape(-1, 1) if len(Y.shape) == 1 else Y\n",
    "\n",
    "        N, M = X.shape\n",
    "        self.out_dims = Y.shape[1]\n",
    "        self.learners = np.empty((self.n_iter, self.out_dims), dtype=object)\n",
    "        self.weights = np.ones((self.n_iter, self.out_dims))\n",
    "        self.weights[1:, :] *= self.learning_rate\n",
    "\n",
    "        Y_pred = np.zeros((N, self.out_dims))\n",
    "        for k in range(self.out_dims):\n",
    "            t = loss.base_estimator()\n",
    "            t.fit(X, Y[:, k])\n",
    "            Y_pred[:, k] += t.predict(X)\n",
    "            self.learners[0, k] = t\n",
    "\n",
    "        for i in range(1, self.n_iter):\n",
    "            for k in range(self.out_dims):\n",
    "                y, y_pred = Y[:, k], Y_pred[:, k]\n",
    "                neg_grad = -1 * loss.grad(y, y_pred)\n",
    "\n",
    "                t = DecisionTree(classifier=False, max_depth=self.max_depth, criterion=\"mse\")\n",
    "                t.fit(X, neg_grad)\n",
    "                self.learners[i, k] = t\n",
    "\n",
    "                step = 1.0\n",
    "                h_pred = t.predict(X)\n",
    "                if self.step_size == \"adaptive\":\n",
    "                    step = loss.line_search(y, y_pred, h_pred)\n",
    "\n",
    "                self.weights[i, k] *= step\n",
    "                Y_pred[:, k] += self.weights[i, k] * h_pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y_pred = np.zeros((X.shape[0], self.out_dims))\n",
    "        for i in range(self.n_iter):\n",
    "            for k in range(self.out_dims):\n",
    "                Y_pred[:, k] += self.weights[i, k] * self.learners[i, k].predict(X)\n",
    "\n",
    "        if self.classifier:\n",
    "            Y_pred = Y_pred.argmax(axis=1)\n",
    "\n",
    "        return Y_pred\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, classifier=True):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    if classifier:  \n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y, y_pred),\n",
    "            \"Precision\": precision_score(y, y_pred, average=\"weighted\"),\n",
    "            \"Recall\": recall_score(y, y_pred, average=\"weighted\"),\n",
    "            \"F1 Score\": f1_score(y, y_pred, average=\"weighted\"),\n",
    "        }\n",
    "    else:  \n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y) - 1) / (len(y) - X.shape[1] - 1)\n",
    "        metrics = {\n",
    "            \"MAE\": mean_absolute_error(y, y_pred),\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": np.sqrt(mse),\n",
    "            \"R2 Square\": r2,\n",
    "            \"Adj R Square\": adj_r2,\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(metrics.items(), columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "\n",
    "def cross_validate(model, X, y, cv=\"kfold\", n_splits=5, classifier=True):\n",
    "    if cv == \"kfold\":\n",
    "        cv_splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"stratified\":\n",
    "        if not classifier:\n",
    "            raise ValueError(\"Stratified K-Fold only works for classification tasks.\")\n",
    "        cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"loo\":\n",
    "        cv_splitter = LeaveOneOut()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cv type. Choose from 'kfold', 'stratified', 'loo'.\")\n",
    "\n",
    "    metrics_list = []\n",
    "    for train_idx, val_idx in cv_splitter.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_val, y_val, classifier=classifier)\n",
    "        metrics_list.append(metrics.set_index(\"Metric\")[\"Value\"])\n",
    "\n",
    "    avg_metrics = pd.DataFrame(metrics_list).mean(axis=0)\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    X_class = np.random.randn(200, 2)\n",
    "    y_class = (X_class[:, 0] + X_class[:, 1] > 0).astype(int)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_class, y_class, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    custom_gbm = GradientBoostedDecisionTree(\n",
    "        n_iter=10, max_depth=3, classifier=True, learning_rate=0.1, loss=\"crossentropy\"\n",
    "    )\n",
    "    custom_gbm.fit(X_train, y_train)\n",
    "\n",
    "    sklearn_gbm = GradientBoostingClassifier(\n",
    "        n_estimators=10, max_depth=3, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    sklearn_gbm.fit(X_train, y_train)\n",
    "\n",
    "    custom_train_metrics = evaluate_model(custom_gbm, X_train, y_train, classifier=True)\n",
    "    custom_val_metrics = evaluate_model(custom_gbm, X_val, y_val, classifier=True)\n",
    "    custom_test_metrics = evaluate_model(custom_gbm, X_test, y_test, classifier=True)\n",
    "\n",
    "    sklearn_train_metrics = evaluate_model(sklearn_gbm, X_train, y_train, classifier=True)\n",
    "    sklearn_val_metrics = evaluate_model(sklearn_gbm, X_val, y_val, classifier=True)\n",
    "    sklearn_test_metrics = evaluate_model(sklearn_gbm, X_test, y_test, classifier=True)\n",
    "\n",
    "    print(\"Classification Train Metrics Comparison:\")\n",
    "    print(pd.concat([custom_train_metrics, sklearn_train_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nClassification Validation Metrics Comparison:\")\n",
    "    print(pd.concat([custom_val_metrics, sklearn_val_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nClassification Test Metrics Comparison:\")\n",
    "    print(pd.concat([custom_test_metrics, sklearn_test_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    custom_cv_metrics = cross_validate(custom_gbm, X_class, y_class, cv=\"stratified\", n_splits=5, classifier=True)\n",
    "    sklearn_cv_metrics = cross_validate(sklearn_gbm, X_class, y_class, cv=\"stratified\", n_splits=5, classifier=True)\n",
    "\n",
    "    print(\"\\nClassification Cross-Validation Metrics Comparison:\")\n",
    "    print(pd.concat([custom_cv_metrics, sklearn_cv_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    X_reg = np.random.rand(200, 2) * 10\n",
    "    y_reg = 3 * X_reg[:, 0] + 2 * X_reg[:, 1] + np.random.randn(200)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_reg, y_reg, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    custom_gbm = GradientBoostedDecisionTree(\n",
    "        n_iter=10, max_depth=3, classifier=False, learning_rate=0.1, loss=\"mse\"\n",
    "    )\n",
    "    custom_gbm.fit(X_train, y_train)\n",
    "\n",
    "    sklearn_gbm = GradientBoostingRegressor(\n",
    "        n_estimators=10, max_depth=3, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    sklearn_gbm.fit(X_train, y_train)\n",
    "\n",
    "    custom_train_metrics = evaluate_model(custom_gbm, X_train, y_train, classifier=False)\n",
    "    custom_val_metrics = evaluate_model(custom_gbm, X_val, y_val, classifier=False)\n",
    "    custom_test_metrics = evaluate_model(custom_gbm, X_test, y_test, classifier=False)\n",
    "\n",
    "    sklearn_train_metrics = evaluate_model(sklearn_gbm, X_train, y_train, classifier=False)\n",
    "    sklearn_val_metrics = evaluate_model(sklearn_gbm, X_val, y_val, classifier=False)\n",
    "    sklearn_test_metrics = evaluate_model(sklearn_gbm, X_test, y_test, classifier=False)\n",
    "\n",
    "    print(\"\\nRegression Train Metrics Comparison:\")\n",
    "    print(pd.concat([custom_train_metrics, sklearn_train_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nRegression Validation Metrics Comparison:\")\n",
    "    print(pd.concat([custom_val_metrics, sklearn_val_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    print(\"\\nRegression Test Metrics Comparison:\")\n",
    "    print(pd.concat([custom_test_metrics, sklearn_test_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    custom_cv_metrics = cross_validate(custom_gbm, X_reg, y_reg, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "    sklearn_cv_metrics = cross_validate(sklearn_gbm, X_reg, y_reg, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "\n",
    "    print(\"\\nRegression Cross-Validation Metrics Comparison:\")\n",
    "    print(pd.concat([custom_cv_metrics, sklearn_cv_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cvxopt\n",
      "  Downloading cvxopt-1.3.2-cp39-cp39-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading cvxopt-1.3.2-cp39-cp39-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 16.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/12.8 MB 23.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.9/12.8 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 21.2 MB/s eta 0:00:00\n",
      "Installing collected packages: cvxopt\n",
      "Successfully installed cvxopt-1.3.2\n"
     ]
    }
   ],
   "source": [
    "! pip install cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from cvxopt import matrix, solvers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_clf(X, y, cls):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = cls.predict(points).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", marker='o')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, C=1, sigma=1, epsilon=0.1, kernel=\"linear\", task=\"classification\", use_smo=True, max_iter=1000):\n",
    "        assert kernel in [\"linear\", \"gaussian\"], \"Unsupported kernel type!\"\n",
    "        assert task in [\"classification\", \"regression\"], \"Unsupported task type!\"\n",
    "        self.C = C\n",
    "        self.sigma = sigma\n",
    "        self.epsilon = epsilon\n",
    "        self.kernel_type = kernel\n",
    "        self.task = task\n",
    "        self.use_smo = use_smo  \n",
    "        self.max_iter = max_iter  \n",
    "        self.b = 0\n",
    "        self.alpha = None\n",
    "\n",
    "        # 核函数选择\n",
    "        if kernel == \"linear\":\n",
    "            self.kernel = lambda x, z: np.dot(x, z)\n",
    "        elif kernel == \"gaussian\":\n",
    "            self.kernel = lambda x, z: np.exp(-0.5 * np.linalg.norm(x - z) ** 2 / (sigma ** 2))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.K = np.array([[self.kernel(X[i], X[j]) for j in range(n_samples)] for i in range(n_samples)])\n",
    "        self.K = np.clip(self.K, -1e5, 1e5)  \n",
    "        if self.use_smo:\n",
    "            max_iter = 500  \n",
    "            for iter_ in range(max_iter):\n",
    "                alpha_prev = np.copy(self.alpha)\n",
    "                for i in range(n_samples):\n",
    "                    for j in range(i + 1, n_samples):  \n",
    "                        self.update(i, j)\n",
    "                norm_diff = np.linalg.norm(self.alpha - alpha_prev)\n",
    "                if iter_ % 10 == 0:\n",
    "                    print(f\"Iteration {iter_ + 1}, ||alpha - alpha_prev|| = {norm_diff:.5f}\")\n",
    "                if norm_diff < 1e-5:\n",
    "                    print(f\"Converged after {iter_ + 1} iterations.\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"SMO optimization did not converge within the maximum iterations.\")\n",
    "        else:\n",
    "            P = matrix(self.K * np.outer(y, y))\n",
    "            q = matrix(-np.ones(n_samples))\n",
    "            G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
    "            h = matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * self.C)))\n",
    "            A = matrix(y.reshape(1, -1).astype(\"double\"))\n",
    "            b = matrix(np.zeros(1))\n",
    "\n",
    "            sol = solvers.qp(P, q, G, h, A, b)\n",
    "            self.alpha = np.ravel(sol[\"x\"])\n",
    "\n",
    "        support_vector_idx = np.where((self.alpha > 1e-4) & (self.alpha < self.C))[0]\n",
    "        if len(support_vector_idx) > 0:\n",
    "            self.b = np.mean([\n",
    "                y[i] - np.clip(np.sum(self.alpha * y * self.K[i]), -1e5, 1e5)\n",
    "                for i in support_vector_idx\n",
    "            ])\n",
    "        else:\n",
    "            print(\"Warning: No support vectors found. The model might not be properly trained.\")\n",
    "            self.b = 0\n",
    "\n",
    "\n",
    "    def update(self, i, j):\n",
    "        if i == j:\n",
    "            return\n",
    "\n",
    "        a_i, a_j = self.alpha[i], self.alpha[j]\n",
    "\n",
    "        if self.task == \"classification\":\n",
    "            L = max(0, a_j - a_i) if self.y[i] != self.y[j] else max(0, a_i + a_j - self.C)\n",
    "            H = min(self.C, self.C + a_j - a_i) if self.y[i] != self.y[j] else min(self.C, a_i + a_j)\n",
    "        elif self.task == \"regression\":\n",
    "            L = max(0, a_j - self.epsilon)\n",
    "            H = min(self.C, a_j + self.epsilon)\n",
    "\n",
    "        eta = self.K[i, i] + self.K[j, j] - 2 * self.K[i, j]\n",
    "        if eta <= 1e-10:  \n",
    "            return\n",
    "\n",
    "        a_j_unc = a_j + self.y[j] * (self._E(i) - self._E(j)) / eta\n",
    "        a_j_new = np.clip(a_j_unc, L, H) \n",
    "\n",
    "        a_i_new = a_i + self.y[i] * self.y[j] * (a_j - a_j_new)\n",
    "\n",
    "        self.alpha[i] = a_i_new\n",
    "        self.alpha[j] = a_j_new\n",
    "\n",
    "        b_i_new = self.y[i] - np.clip(np.sum(self.alpha * self.y * self.K[i]), -1e5, 1e5)\n",
    "        b_j_new = self.y[j] - np.clip(np.sum(self.alpha * self.y * self.K[j]), -1e5, 1e5)\n",
    "        self.b = (b_i_new + b_j_new) / 2\n",
    "\n",
    "    def _E(self, i):\n",
    "        return self._g(i) - self.y[i]\n",
    "\n",
    "    def _g(self, i):\n",
    "        return np.clip(np.sum(self.alpha * self.y * self.K[:, i]) + self.b, -1e5, 1e5)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in tqdm(X):\n",
    "            pred = np.sum(\n",
    "                self.alpha * self.y * np.array([self.kernel(x, self.X[j]) for j in range(len(self.X))])\n",
    "            ) + self.b\n",
    "            if self.task == \"classification\":\n",
    "                preds.append(1 if pred >= 0 else -1)\n",
    "            elif self.task == \"regression\":\n",
    "                preds.append(np.clip(pred, -1e5, 1e5))\n",
    "        return np.array(preds)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        if self.task == \"classification\":\n",
    "            return np.mean(preds == y)\n",
    "        elif self.task == \"regression\":\n",
    "            return r2_score(y, preds)\n",
    "\n",
    "def evaluate_model(model, X, y, classifier=True):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    if classifier:  \n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y, y_pred),\n",
    "            \"Precision\": precision_score(y, y_pred, average=\"weighted\"),\n",
    "            \"Recall\": recall_score(y, y_pred, average=\"weighted\"),\n",
    "            \"F1 Score\": f1_score(y, y_pred, average=\"weighted\"),\n",
    "        }\n",
    "    else: \n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y) - 1) / (len(y) - X.shape[1] - 1)\n",
    "        metrics = {\n",
    "            \"MAE\": mean_absolute_error(y, y_pred),\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": np.sqrt(mse),\n",
    "            \"R2 Square\": r2,\n",
    "            \"Adj R Square\": adj_r2,\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(metrics.items(), columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "def cross_validate(model, X, y, cv=\"kfold\", n_splits=5, classifier=True):\n",
    "    if cv == \"kfold\":\n",
    "        cv_splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"stratified\":\n",
    "        if not classifier:\n",
    "            raise ValueError(\"Stratified K-Fold only works for classification tasks.\")\n",
    "        cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"loo\":\n",
    "        cv_splitter = LeaveOneOut()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cv type. Choose from 'kfold', 'stratified', 'loo'.\")\n",
    "\n",
    "    metrics_list = []\n",
    "    for train_idx, val_idx in cv_splitter.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_val, y_val, classifier=classifier)\n",
    "        metrics_list.append(metrics.set_index(\"Metric\")[\"Value\"])\n",
    "\n",
    "    avg_metrics = pd.DataFrame(metrics_list).mean(axis=0)\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 34 iterations.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1390e+01 -3.5996e+02  2e+03  4e+00  2e-15\n",
      " 1: -2.3993e+01 -2.4848e+02  4e+02  5e-01  2e-15\n",
      " 2: -9.9620e+00 -6.3950e+01  7e+01  4e-02  1e-14\n",
      " 3: -1.2894e+01 -2.3000e+01  1e+01  6e-03  3e-15\n",
      " 4: -1.5814e+01 -1.8478e+01  3e+00  1e-03  1e-15\n",
      " 5: -1.6600e+01 -1.7261e+01  7e-01  3e-04  1e-15\n",
      " 6: -1.6823e+01 -1.6985e+01  2e-01  5e-05  1e-15\n",
      " 7: -1.6884e+01 -1.6909e+01  2e-02  5e-07  1e-15\n",
      " 8: -1.6896e+01 -1.6897e+01  2e-03  2e-08  2e-15\n",
      " 9: -1.6896e+01 -1.6896e+01  2e-05  2e-10  2e-15\n",
      "10: -1.6896e+01 -1.6896e+01  2e-07  2e-12  1e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 12620.14it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 9879.41it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 10106.76it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 14026.21it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 13366.17it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 7832.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Train Metrics Comparison:\n",
      "  Custom SVM (SMO)       Custom SVM (No SMO)       Sklearn SVM      \n",
      "            Metric Value              Metric Value      Metric Value\n",
      "0         Accuracy   1.0            Accuracy   1.0    Accuracy   1.0\n",
      "1        Precision   1.0           Precision   1.0   Precision   1.0\n",
      "2           Recall   1.0              Recall   1.0      Recall   1.0\n",
      "3         F1 Score   1.0            F1 Score   1.0    F1 Score   1.0\n",
      "\n",
      "Classification Validation Metrics Comparison:\n",
      "  Custom SVM (SMO)       Custom SVM (No SMO)       Sklearn SVM      \n",
      "            Metric Value              Metric Value      Metric Value\n",
      "0         Accuracy   1.0            Accuracy   1.0    Accuracy   1.0\n",
      "1        Precision   1.0           Precision   1.0   Precision   1.0\n",
      "2           Recall   1.0              Recall   1.0      Recall   1.0\n",
      "3         F1 Score   1.0            F1 Score   1.0    F1 Score   1.0\n",
      "\n",
      "Classification Test Metrics Comparison:\n",
      "  Custom SVM (SMO)       Custom SVM (No SMO)       Sklearn SVM      \n",
      "            Metric Value              Metric Value      Metric Value\n",
      "0         Accuracy   1.0            Accuracy   1.0    Accuracy   1.0\n",
      "1        Precision   1.0           Precision   1.0   Precision   1.0\n",
      "2           Recall   1.0              Recall   1.0      Recall   1.0\n",
      "3         F1 Score   1.0            F1 Score   1.0    F1 Score   1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 55 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 27 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 10 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 7788.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 16 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 22 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 3154.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4595e+01 -4.4957e+02  3e+03  3e+00  4e-15\n",
      " 1: -3.2549e+01 -3.0316e+02  5e+02  5e-01  3e-15\n",
      " 2: -1.4558e+01 -8.0704e+01  9e+01  5e-02  6e-15\n",
      " 3: -1.6280e+01 -2.8960e+01  1e+01  8e-03  2e-15\n",
      " 4: -1.9605e+01 -2.2834e+01  4e+00  1e-03  1e-15\n",
      " 5: -2.0559e+01 -2.1366e+01  8e-01  1e-04  2e-15\n",
      " 6: -2.0732e+01 -2.1144e+01  4e-01  2e-05  1e-15\n",
      " 7: -2.0914e+01 -2.0928e+01  1e-02  7e-07  2e-15\n",
      " 8: -2.0920e+01 -2.0922e+01  3e-03  9e-08  2e-15\n",
      " 9: -2.0921e+01 -2.0921e+01  3e-05  1e-09  2e-15\n",
      "10: -2.0921e+01 -2.0921e+01  3e-07  1e-11  2e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.6588e+01 -4.8320e+02  3e+03  4e+00  4e-15\n",
      " 1: -3.3646e+01 -3.3469e+02  6e+02  5e-01  3e-15\n",
      " 2: -1.5434e+01 -9.5720e+01  1e+02  6e-02  8e-15\n",
      " 3: -1.6467e+01 -3.0471e+01  2e+01  8e-03  3e-15\n",
      " 4: -2.0051e+01 -2.4332e+01  5e+00  2e-03  1e-15\n",
      " 5: -2.0895e+01 -2.3035e+01  2e+00  8e-04  1e-15\n",
      " 6: -2.1568e+01 -2.2039e+01  5e-01  1e-04  1e-15\n",
      " 7: -2.1735e+01 -2.1807e+01  8e-02  1e-05  2e-15\n",
      " 8: -2.1766e+01 -2.1770e+01  4e-03  2e-07  1e-15\n",
      " 9: -2.1768e+01 -2.1768e+01  4e-05  2e-09  1e-15\n",
      "10: -2.1768e+01 -2.1768e+01  4e-07  2e-11  2e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.5817e+01 -4.7370e+02  3e+03  4e+00  5e-15\n",
      " 1: -3.2566e+01 -3.2497e+02  6e+02  5e-01  3e-15\n",
      " 2: -1.4628e+01 -9.1021e+01  1e+02  6e-02  5e-15\n",
      " 3: -1.4915e+01 -2.8513e+01  2e+01  8e-03  2e-15\n",
      " 4: -1.8227e+01 -2.1840e+01  4e+00  2e-03  1e-15\n",
      " 5: -1.9121e+01 -2.0494e+01  2e+00  6e-04  1e-15\n",
      " 6: -1.9500e+01 -1.9975e+01  5e-01  2e-04  1e-15\n",
      " 7: -1.9669e+01 -1.9742e+01  8e-02  1e-05  1e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8: -1.9702e+01 -1.9704e+01  1e-03  2e-07  1e-15\n",
      " 9: -1.9703e+01 -1.9703e+01  1e-05  2e-09  2e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0: -5.9853e+01 -4.9033e+02  3e+03  4e+00  4e-15\n",
      " 1: -3.5136e+01 -3.4304e+02  6e+02  5e-01  3e-15\n",
      " 2: -1.5090e+01 -1.0523e+02  1e+02  7e-02  6e-15\n",
      " 3: -1.5423e+01 -3.2510e+01  2e+01  1e-02  3e-15\n",
      " 4: -1.9168e+01 -2.4903e+01  7e+00  3e-03  2e-15\n",
      " 5: -2.0556e+01 -2.2563e+01  2e+00  8e-04  1e-15\n",
      " 6: -2.1015e+01 -2.1869e+01  9e-01  3e-04  1e-15\n",
      " 7: -2.1259e+01 -2.1502e+01  3e-01  7e-05  1e-15\n",
      " 8: -2.1357e+01 -2.1369e+01  1e-02  2e-06  2e-15\n",
      " 9: -2.1362e+01 -2.1363e+01  1e-04  2e-08  2e-15\n",
      "10: -2.1362e+01 -2.1362e+01  1e-06  2e-10  2e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0: -5.4818e+01 -4.8015e+02  3e+03  4e+00  4e-15\n",
      " 1: -3.1900e+01 -3.3120e+02  6e+02  5e-01  3e-15\n",
      " 2: -1.3807e+01 -9.7501e+01  1e+02  7e-02  6e-15\n",
      " 3: -1.3406e+01 -2.9663e+01  2e+01  1e-02  2e-15\n",
      " 4: -1.6683e+01 -2.1728e+01  6e+00  3e-03  1e-15\n",
      " 5: -1.7924e+01 -1.9585e+01  2e+00  6e-04  1e-15\n",
      " 6: -1.8292e+01 -1.8956e+01  7e-01  2e-04  1e-15\n",
      " 7: -1.8513e+01 -1.8660e+01  2e-01  4e-05  1e-15\n",
      " 8: -1.8570e+01 -1.8582e+01  1e-02  7e-16  1e-15\n",
      " 9: -1.8576e+01 -1.8576e+01  2e-04  7e-16  2e-15\n",
      "10: -1.8576e+01 -1.8576e+01  2e-06  1e-16  2e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 9834.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Cross-Validation Metrics Comparison:\n",
      "           Custom SVM (SMO)  Custom SVM (No SMO)  Sklearn SVM\n",
      "Metric                                                       \n",
      "Accuracy           0.995000                  1.0     0.995000\n",
      "Precision          0.995238                  1.0     0.995238\n",
      "Recall             0.995000                  1.0     0.995000\n",
      "F1 Score           0.994997                  1.0     0.994997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86173\\AppData\\Local\\Temp\\ipykernel_42764\\3383946919.py:125: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  a_j_unc = a_j + self.y[j] * (self._E(i) - self._E(j)) / eta\n",
      "C:\\Users\\86173\\AppData\\Local\\Temp\\ipykernel_42764\\3383946919.py:136: RuntimeWarning: overflow encountered in multiply\n",
      "  b_i_new = self.y[i] - np.sum(self.alpha * self.y * self.K[i])\n",
      "C:\\Users\\86173\\AppData\\Local\\Temp\\ipykernel_42764\\3383946919.py:137: RuntimeWarning: overflow encountered in multiply\n",
      "  b_j_new = self.y[j] - np.sum(self.alpha * self.y * self.K[j])\n",
      "C:\\Users\\86173\\AppData\\Local\\Temp\\ipykernel_42764\\3383946919.py:151: RuntimeWarning: overflow encountered in multiply\n",
      "  return np.sum(self.alpha * self.y * self.K[:, i]) + self.b\n",
      "C:\\Users\\86173\\AppData\\Local\\Temp\\ipykernel_42764\\3383946919.py:151: RuntimeWarning: invalid value encountered in scalar add\n",
      "  return np.sum(self.alpha * self.y * self.K[:, i]) + self.b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# 自定义 SVM 回归 (使用 SMO 优化)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m custom_svr_smo \u001b[38;5;241m=\u001b[39m SVM(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_smo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 80\u001b[0m \u001b[43mcustom_svr_smo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# 自定义 SVM 回归 (不使用 SMO 优化)\u001b[39;00m\n\u001b[0;32m     83\u001b[0m custom_svr_no_smo \u001b[38;5;241m=\u001b[39m SVM(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_smo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[27], line 74\u001b[0m, in \u001b[0;36mSVM.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[1;32m---> 74\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# 判断是否收敛\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m-\u001b[39m alpha_prev) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-5\u001b[39m:\n",
      "Cell \u001b[1;32mIn[27], line 137\u001b[0m, in \u001b[0;36mSVM.update\u001b[1;34m(self, i, j)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# 更新偏置项 b\u001b[39;00m\n\u001b[0;32m    136\u001b[0m b_i_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[i] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK[i])\n\u001b[1;32m--> 137\u001b[0m b_j_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[j] \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m (b_i_new \u001b[38;5;241m+\u001b[39m b_j_new) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    X_class = np.random.randn(200, 2)\n",
    "    y_class = (X_class[:, 0] + X_class[:, 1] > 0).astype(int) * 2 - 1  \n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_class, y_class, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    custom_svm_smo = SVM(C=1, kernel=\"linear\", task=\"classification\", use_smo=True)\n",
    "    custom_svm_smo.fit(X_train, y_train)\n",
    "\n",
    "    custom_svm_no_smo = SVM(C=1, kernel=\"linear\", task=\"classification\", use_smo=False)\n",
    "    custom_svm_no_smo.fit(X_train, y_train)\n",
    "\n",
    "    sklearn_svm = SVC(C=1, kernel=\"linear\", random_state=42)\n",
    "    sklearn_svm.fit(X_train, y_train)\n",
    "\n",
    "    custom_train_metrics_smo = evaluate_model(custom_svm_smo, X_train, y_train, classifier=True)\n",
    "    custom_val_metrics_smo = evaluate_model(custom_svm_smo, X_val, y_val, classifier=True)\n",
    "    custom_test_metrics_smo = evaluate_model(custom_svm_smo, X_test, y_test, classifier=True)\n",
    "\n",
    "    custom_train_metrics_no_smo = evaluate_model(custom_svm_no_smo, X_train, y_train, classifier=True)\n",
    "    custom_val_metrics_no_smo = evaluate_model(custom_svm_no_smo, X_val, y_val, classifier=True)\n",
    "    custom_test_metrics_no_smo = evaluate_model(custom_svm_no_smo, X_test, y_test, classifier=True)\n",
    "\n",
    "    sklearn_train_metrics = evaluate_model(sklearn_svm, X_train, y_train, classifier=True)\n",
    "    sklearn_val_metrics = evaluate_model(sklearn_svm, X_val, y_val, classifier=True)\n",
    "    sklearn_test_metrics = evaluate_model(sklearn_svm, X_test, y_test, classifier=True)\n",
    "\n",
    "    print(\"Classification Train Metrics Comparison:\")\n",
    "    print(pd.concat(\n",
    "        [custom_train_metrics_smo, custom_train_metrics_no_smo, sklearn_train_metrics],\n",
    "        axis=1,\n",
    "        keys=[\"Custom SVM (SMO)\", \"Custom SVM (No SMO)\", \"Sklearn SVM\"]\n",
    "    ))\n",
    "\n",
    "    print(\"\\nClassification Validation Metrics Comparison:\")\n",
    "    print(pd.concat(\n",
    "        [custom_val_metrics_smo, custom_val_metrics_no_smo, sklearn_val_metrics],\n",
    "        axis=1,\n",
    "        keys=[\"Custom SVM (SMO)\", \"Custom SVM (No SMO)\", \"Sklearn SVM\"]\n",
    "    ))\n",
    "\n",
    "    print(\"\\nClassification Test Metrics Comparison:\")\n",
    "    print(pd.concat(\n",
    "        [custom_test_metrics_smo, custom_test_metrics_no_smo, sklearn_test_metrics],\n",
    "        axis=1,\n",
    "        keys=[\"Custom SVM (SMO)\", \"Custom SVM (No SMO)\", \"Sklearn SVM\"]\n",
    "    ))\n",
    "\n",
    "    custom_cv_metrics_smo = cross_validate(custom_svm_smo, X_class, y_class, cv=\"stratified\", n_splits=5, classifier=True)\n",
    "    custom_cv_metrics_no_smo = cross_validate(custom_svm_no_smo, X_class, y_class, cv=\"stratified\", n_splits=5, classifier=True)\n",
    "    sklearn_cv_metrics = cross_validate(sklearn_svm, X_class, y_class, cv=\"stratified\", n_splits=5, classifier=True)\n",
    "\n",
    "    print(\"\\nClassification Cross-Validation Metrics Comparison:\")\n",
    "    print(pd.concat(\n",
    "        [custom_cv_metrics_smo, custom_cv_metrics_no_smo, sklearn_cv_metrics],\n",
    "        axis=1,\n",
    "        keys=[\"Custom SVM (SMO)\", \"Custom SVM (No SMO)\", \"Sklearn SVM\"]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, ||alpha - alpha_prev|| = 9.37166\n",
      "Iteration 11, ||alpha - alpha_prev|| = 2.26468\n",
      "Iteration 21, ||alpha - alpha_prev|| = 3.05084\n",
      "Iteration 31, ||alpha - alpha_prev|| = 3.87264\n",
      "Iteration 41, ||alpha - alpha_prev|| = 5.08330\n",
      "Iteration 51, ||alpha - alpha_prev|| = 3.91108\n",
      "Iteration 61, ||alpha - alpha_prev|| = 5.38587\n",
      "Iteration 71, ||alpha - alpha_prev|| = 5.58920\n",
      "Iteration 81, ||alpha - alpha_prev|| = 4.70576\n",
      "Iteration 91, ||alpha - alpha_prev|| = 5.31358\n",
      "Iteration 101, ||alpha - alpha_prev|| = 4.81138\n",
      "Iteration 111, ||alpha - alpha_prev|| = 5.89473\n",
      "Iteration 121, ||alpha - alpha_prev|| = 5.20858\n",
      "Iteration 131, ||alpha - alpha_prev|| = 4.73818\n",
      "Iteration 141, ||alpha - alpha_prev|| = 4.21163\n",
      "Iteration 151, ||alpha - alpha_prev|| = 4.56944\n",
      "Iteration 161, ||alpha - alpha_prev|| = 5.71511\n",
      "Iteration 171, ||alpha - alpha_prev|| = 6.15716\n",
      "Iteration 181, ||alpha - alpha_prev|| = 7.49054\n",
      "Iteration 191, ||alpha - alpha_prev|| = 5.32540\n",
      "Iteration 201, ||alpha - alpha_prev|| = 6.69030\n",
      "Iteration 211, ||alpha - alpha_prev|| = 5.76840\n",
      "Iteration 221, ||alpha - alpha_prev|| = 5.05717\n",
      "Iteration 231, ||alpha - alpha_prev|| = 6.58710\n",
      "Iteration 241, ||alpha - alpha_prev|| = 5.13543\n",
      "Iteration 251, ||alpha - alpha_prev|| = 5.03783\n",
      "Iteration 261, ||alpha - alpha_prev|| = 5.70906\n",
      "Iteration 271, ||alpha - alpha_prev|| = 6.10356\n",
      "Iteration 281, ||alpha - alpha_prev|| = 5.26403\n",
      "Iteration 291, ||alpha - alpha_prev|| = 5.70495\n",
      "Iteration 301, ||alpha - alpha_prev|| = 5.23714\n",
      "Iteration 311, ||alpha - alpha_prev|| = 6.15153\n",
      "Iteration 321, ||alpha - alpha_prev|| = 5.68792\n",
      "Iteration 331, ||alpha - alpha_prev|| = 5.67854\n",
      "Iteration 341, ||alpha - alpha_prev|| = 5.14600\n",
      "Iteration 351, ||alpha - alpha_prev|| = 5.43082\n",
      "Iteration 361, ||alpha - alpha_prev|| = 4.86570\n",
      "Iteration 371, ||alpha - alpha_prev|| = 5.85521\n",
      "Iteration 381, ||alpha - alpha_prev|| = 6.08194\n",
      "Iteration 391, ||alpha - alpha_prev|| = 6.26994\n",
      "Iteration 401, ||alpha - alpha_prev|| = 5.03044\n",
      "Iteration 411, ||alpha - alpha_prev|| = 5.37823\n",
      "Iteration 421, ||alpha - alpha_prev|| = 5.41188\n",
      "Iteration 431, ||alpha - alpha_prev|| = 6.35516\n",
      "Iteration 441, ||alpha - alpha_prev|| = 6.10334\n",
      "Iteration 451, ||alpha - alpha_prev|| = 5.70995\n",
      "Iteration 461, ||alpha - alpha_prev|| = 7.10702\n",
      "Iteration 471, ||alpha - alpha_prev|| = 6.97915\n",
      "Iteration 481, ||alpha - alpha_prev|| = 4.66436\n",
      "Iteration 491, ||alpha - alpha_prev|| = 5.09368\n",
      "SMO optimization did not converge within the maximum iterations.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.1714e+01 -2.8772e+02  1e+03  3e+00  3e-15\n",
      " 1: -4.2529e+01 -1.9241e+02  3e+02  4e-01  3e-15\n",
      " 2: -3.3486e+01 -8.3832e+01  8e+01  1e-01  5e-15\n",
      " 3: -3.2358e+01 -5.1590e+01  3e+01  3e-02  2e-15\n",
      " 4: -3.3653e+01 -3.8587e+01  7e+00  8e-03  2e-15\n",
      " 5: -3.4155e+01 -3.5706e+01  2e+00  2e-03  2e-15\n",
      " 6: -3.4507e+01 -3.4699e+01  2e-01  5e-05  2e-15\n",
      " 7: -3.4570e+01 -3.4605e+01  4e-02  8e-06  2e-15\n",
      " 8: -3.4584e+01 -3.4585e+01  9e-04  7e-08  2e-15\n",
      " 9: -3.4585e+01 -3.4585e+01  9e-06  7e-10  2e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 10152.01it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 7926.49it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 7836.52it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 7626.12it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 7025.05it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 9194.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression Train Metrics Comparison:\n",
      "  Custom SVR (SMO)           Custom SVR (No SMO)             Sklearn SVR  \\\n",
      "            Metric     Value              Metric     Value        Metric   \n",
      "0              MAE  0.071538                 MAE  2.482207           MAE   \n",
      "1              MSE  0.008375                 MSE  8.690162           MSE   \n",
      "2             RMSE  0.091514                RMSE  2.947908          RMSE   \n",
      "3        R2 Square  0.991286           R2 Square -8.042367     R2 Square   \n",
      "4     Adj R Square  0.991137        Adj R Square -8.196937  Adj R Square   \n",
      "\n",
      "             \n",
      "      Value  \n",
      "0  0.072590  \n",
      "1  0.007969  \n",
      "2  0.089271  \n",
      "3  0.991708  \n",
      "4  0.991566  \n",
      "\n",
      "Regression Validation Metrics Comparison:\n",
      "  Custom SVR (SMO)           Custom SVR (No SMO)             Sklearn SVR  \\\n",
      "            Metric     Value              Metric     Value        Metric   \n",
      "0              MAE  0.085649                 MAE  2.792963           MAE   \n",
      "1              MSE  0.011859                 MSE  9.958371           MSE   \n",
      "2             RMSE  0.108897                RMSE  3.155689          RMSE   \n",
      "3        R2 Square  0.989814           R2 Square -7.553924     R2 Square   \n",
      "4     Adj R Square  0.989263        Adj R Square -8.016298  Adj R Square   \n",
      "\n",
      "             \n",
      "      Value  \n",
      "0  0.083844  \n",
      "1  0.011510  \n",
      "2  0.107287  \n",
      "3  0.990113  \n",
      "4  0.989578  \n",
      "\n",
      "Regression Test Metrics Comparison:\n",
      "  Custom SVR (SMO)           Custom SVR (No SMO)             Sklearn SVR  \\\n",
      "            Metric     Value              Metric     Value        Metric   \n",
      "0              MAE  0.076340                 MAE  2.331837           MAE   \n",
      "1              MSE  0.010412                 MSE  8.224424           MSE   \n",
      "2             RMSE  0.102042                RMSE  2.867826          RMSE   \n",
      "3        R2 Square  0.987519           R2 Square -8.857903     R2 Square   \n",
      "4     Adj R Square  0.986845        Adj R Square -9.390762  Adj R Square   \n",
      "\n",
      "             \n",
      "      Value  \n",
      "0  0.075595  \n",
      "1  0.010201  \n",
      "2  0.101001  \n",
      "3  0.987773  \n",
      "4  0.987112  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, ||alpha - alpha_prev|| = 14.04790\n",
      "Iteration 11, ||alpha - alpha_prev|| = 4.11689\n",
      "Iteration 21, ||alpha - alpha_prev|| = 4.11936\n",
      "Iteration 31, ||alpha - alpha_prev|| = 4.14979\n",
      "Iteration 41, ||alpha - alpha_prev|| = 4.09225\n",
      "Iteration 51, ||alpha - alpha_prev|| = 3.83478\n",
      "Iteration 61, ||alpha - alpha_prev|| = 6.30479\n",
      "Iteration 71, ||alpha - alpha_prev|| = 4.33141\n",
      "Iteration 81, ||alpha - alpha_prev|| = 3.68549\n",
      "Iteration 91, ||alpha - alpha_prev|| = 4.29734\n",
      "Iteration 101, ||alpha - alpha_prev|| = 7.05691\n",
      "Iteration 111, ||alpha - alpha_prev|| = 7.04227\n",
      "Iteration 121, ||alpha - alpha_prev|| = 3.68784\n",
      "Iteration 131, ||alpha - alpha_prev|| = 4.84784\n",
      "Iteration 141, ||alpha - alpha_prev|| = 3.89674\n",
      "Iteration 151, ||alpha - alpha_prev|| = 4.14617\n",
      "Iteration 161, ||alpha - alpha_prev|| = 3.59313\n",
      "Iteration 171, ||alpha - alpha_prev|| = 3.72650\n",
      "Iteration 181, ||alpha - alpha_prev|| = 3.56293\n",
      "Iteration 191, ||alpha - alpha_prev|| = 4.21788\n",
      "Iteration 201, ||alpha - alpha_prev|| = 4.99892\n",
      "Iteration 211, ||alpha - alpha_prev|| = 5.07675\n",
      "Iteration 221, ||alpha - alpha_prev|| = 7.31907\n",
      "Iteration 231, ||alpha - alpha_prev|| = 3.77283\n",
      "Iteration 241, ||alpha - alpha_prev|| = 4.43888\n",
      "Iteration 251, ||alpha - alpha_prev|| = 4.34056\n",
      "Iteration 261, ||alpha - alpha_prev|| = 4.51598\n",
      "Iteration 271, ||alpha - alpha_prev|| = 4.47847\n",
      "Iteration 281, ||alpha - alpha_prev|| = 4.25560\n",
      "Iteration 291, ||alpha - alpha_prev|| = 3.95214\n",
      "Iteration 301, ||alpha - alpha_prev|| = 3.72000\n",
      "Iteration 311, ||alpha - alpha_prev|| = 4.14583\n",
      "Iteration 321, ||alpha - alpha_prev|| = 5.08745\n",
      "Iteration 331, ||alpha - alpha_prev|| = 3.75518\n",
      "Iteration 341, ||alpha - alpha_prev|| = 5.21248\n",
      "Iteration 351, ||alpha - alpha_prev|| = 3.91300\n",
      "Iteration 361, ||alpha - alpha_prev|| = 4.99441\n",
      "Iteration 371, ||alpha - alpha_prev|| = 4.23312\n",
      "Iteration 381, ||alpha - alpha_prev|| = 4.68515\n",
      "Iteration 391, ||alpha - alpha_prev|| = 3.68566\n",
      "Iteration 401, ||alpha - alpha_prev|| = 3.95071\n",
      "Iteration 411, ||alpha - alpha_prev|| = 3.66446\n",
      "Iteration 421, ||alpha - alpha_prev|| = 4.37597\n",
      "Iteration 431, ||alpha - alpha_prev|| = 4.87530\n",
      "Iteration 441, ||alpha - alpha_prev|| = 4.01939\n",
      "Iteration 451, ||alpha - alpha_prev|| = 5.57939\n",
      "Iteration 461, ||alpha - alpha_prev|| = 4.46579\n",
      "Iteration 471, ||alpha - alpha_prev|| = 3.94952\n",
      "Iteration 481, ||alpha - alpha_prev|| = 4.49229\n",
      "Iteration 491, ||alpha - alpha_prev|| = 3.81549\n",
      "SMO optimization did not converge within the maximum iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 11402.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, ||alpha - alpha_prev|| = 14.38552\n",
      "Iteration 11, ||alpha - alpha_prev|| = 6.49771\n",
      "Iteration 21, ||alpha - alpha_prev|| = 6.72689\n",
      "Iteration 31, ||alpha - alpha_prev|| = 5.02412\n",
      "Iteration 41, ||alpha - alpha_prev|| = 7.01209\n",
      "Iteration 51, ||alpha - alpha_prev|| = 11.74594\n",
      "Iteration 61, ||alpha - alpha_prev|| = 5.86990\n",
      "Iteration 71, ||alpha - alpha_prev|| = 9.56326\n",
      "Iteration 81, ||alpha - alpha_prev|| = 5.41636\n",
      "Iteration 91, ||alpha - alpha_prev|| = 7.02411\n",
      "Iteration 101, ||alpha - alpha_prev|| = 4.94376\n",
      "Iteration 111, ||alpha - alpha_prev|| = 5.00339\n",
      "Iteration 121, ||alpha - alpha_prev|| = 4.71899\n",
      "Iteration 131, ||alpha - alpha_prev|| = 6.95027\n",
      "Iteration 141, ||alpha - alpha_prev|| = 4.87588\n",
      "Iteration 151, ||alpha - alpha_prev|| = 7.31635\n",
      "Iteration 161, ||alpha - alpha_prev|| = 5.68485\n",
      "Iteration 171, ||alpha - alpha_prev|| = 6.19402\n",
      "Iteration 181, ||alpha - alpha_prev|| = 5.49277\n",
      "Iteration 191, ||alpha - alpha_prev|| = 4.49468\n",
      "Iteration 201, ||alpha - alpha_prev|| = 9.43565\n",
      "Iteration 211, ||alpha - alpha_prev|| = 5.48085\n",
      "Iteration 221, ||alpha - alpha_prev|| = 7.75263\n",
      "Iteration 231, ||alpha - alpha_prev|| = 4.88703\n",
      "Iteration 241, ||alpha - alpha_prev|| = 7.42464\n",
      "Iteration 251, ||alpha - alpha_prev|| = 5.03523\n",
      "Iteration 261, ||alpha - alpha_prev|| = 5.20549\n",
      "Iteration 271, ||alpha - alpha_prev|| = 4.80983\n",
      "Iteration 281, ||alpha - alpha_prev|| = 7.21514\n",
      "Iteration 291, ||alpha - alpha_prev|| = 5.04683\n",
      "Iteration 301, ||alpha - alpha_prev|| = 4.91949\n",
      "Iteration 311, ||alpha - alpha_prev|| = 6.84952\n",
      "Iteration 321, ||alpha - alpha_prev|| = 5.12624\n",
      "Iteration 331, ||alpha - alpha_prev|| = 9.99188\n",
      "Iteration 341, ||alpha - alpha_prev|| = 5.45612\n",
      "Iteration 351, ||alpha - alpha_prev|| = 8.17735\n",
      "Iteration 361, ||alpha - alpha_prev|| = 6.03979\n",
      "Iteration 371, ||alpha - alpha_prev|| = 4.52847\n",
      "Iteration 381, ||alpha - alpha_prev|| = 5.45255\n",
      "Iteration 391, ||alpha - alpha_prev|| = 6.30592\n",
      "Iteration 401, ||alpha - alpha_prev|| = 5.82962\n",
      "Iteration 411, ||alpha - alpha_prev|| = 5.56104\n",
      "Iteration 421, ||alpha - alpha_prev|| = 4.71665\n",
      "Iteration 431, ||alpha - alpha_prev|| = 5.21720\n",
      "Iteration 441, ||alpha - alpha_prev|| = 4.76042\n",
      "Iteration 451, ||alpha - alpha_prev|| = 6.03579\n",
      "Iteration 461, ||alpha - alpha_prev|| = 7.40209\n",
      "Iteration 471, ||alpha - alpha_prev|| = 4.96125\n",
      "Iteration 481, ||alpha - alpha_prev|| = 5.25315\n",
      "Iteration 491, ||alpha - alpha_prev|| = 5.04009\n",
      "SMO optimization did not converge within the maximum iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 7775.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, ||alpha - alpha_prev|| = 14.05749\n",
      "Iteration 11, ||alpha - alpha_prev|| = 6.92034\n",
      "Iteration 21, ||alpha - alpha_prev|| = 5.40077\n",
      "Iteration 31, ||alpha - alpha_prev|| = 5.07370\n",
      "Iteration 41, ||alpha - alpha_prev|| = 4.33270\n",
      "Iteration 51, ||alpha - alpha_prev|| = 5.36851\n",
      "Iteration 61, ||alpha - alpha_prev|| = 4.78959\n",
      "Iteration 71, ||alpha - alpha_prev|| = 5.51178\n",
      "Iteration 81, ||alpha - alpha_prev|| = 7.82686\n",
      "Iteration 91, ||alpha - alpha_prev|| = 10.00042\n",
      "Iteration 101, ||alpha - alpha_prev|| = 10.12495\n",
      "Iteration 111, ||alpha - alpha_prev|| = 11.17680\n",
      "Iteration 121, ||alpha - alpha_prev|| = 8.85500\n",
      "Iteration 131, ||alpha - alpha_prev|| = 8.50228\n",
      "Iteration 141, ||alpha - alpha_prev|| = 9.28632\n",
      "Iteration 151, ||alpha - alpha_prev|| = 9.20441\n",
      "Iteration 161, ||alpha - alpha_prev|| = 11.40853\n",
      "Iteration 171, ||alpha - alpha_prev|| = 10.81222\n",
      "Iteration 181, ||alpha - alpha_prev|| = 10.09366\n",
      "Iteration 191, ||alpha - alpha_prev|| = 10.55535\n",
      "Iteration 201, ||alpha - alpha_prev|| = 9.61831\n",
      "Iteration 211, ||alpha - alpha_prev|| = 8.58559\n",
      "Iteration 221, ||alpha - alpha_prev|| = 8.93078\n",
      "Iteration 231, ||alpha - alpha_prev|| = 7.44342\n",
      "Iteration 241, ||alpha - alpha_prev|| = 9.01739\n",
      "Iteration 251, ||alpha - alpha_prev|| = 9.53993\n",
      "Iteration 261, ||alpha - alpha_prev|| = 7.23363\n",
      "Iteration 271, ||alpha - alpha_prev|| = 5.00492\n",
      "Iteration 281, ||alpha - alpha_prev|| = 6.27393\n",
      "Iteration 291, ||alpha - alpha_prev|| = 6.98205\n",
      "Iteration 301, ||alpha - alpha_prev|| = 7.40488\n",
      "Iteration 311, ||alpha - alpha_prev|| = 5.35594\n",
      "Iteration 321, ||alpha - alpha_prev|| = 6.73889\n",
      "Iteration 331, ||alpha - alpha_prev|| = 6.03431\n",
      "Iteration 341, ||alpha - alpha_prev|| = 5.11381\n",
      "Iteration 351, ||alpha - alpha_prev|| = 8.19505\n",
      "Iteration 361, ||alpha - alpha_prev|| = 6.46154\n",
      "Iteration 371, ||alpha - alpha_prev|| = 9.16433\n",
      "Iteration 381, ||alpha - alpha_prev|| = 5.91549\n",
      "Iteration 391, ||alpha - alpha_prev|| = 7.75067\n",
      "Iteration 401, ||alpha - alpha_prev|| = 8.26147\n",
      "Iteration 411, ||alpha - alpha_prev|| = 6.15731\n",
      "Iteration 421, ||alpha - alpha_prev|| = 8.14713\n",
      "Iteration 431, ||alpha - alpha_prev|| = 6.22162\n",
      "Iteration 441, ||alpha - alpha_prev|| = 6.58477\n",
      "Iteration 451, ||alpha - alpha_prev|| = 5.54075\n",
      "Iteration 461, ||alpha - alpha_prev|| = 6.82817\n",
      "Iteration 471, ||alpha - alpha_prev|| = 8.00314\n",
      "Iteration 481, ||alpha - alpha_prev|| = 8.69333\n",
      "Iteration 491, ||alpha - alpha_prev|| = 9.17000\n",
      "SMO optimization did not converge within the maximum iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 5916.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, ||alpha - alpha_prev|| = 13.62204\n",
      "Iteration 11, ||alpha - alpha_prev|| = 6.10645\n",
      "Iteration 21, ||alpha - alpha_prev|| = 6.36909\n",
      "Iteration 31, ||alpha - alpha_prev|| = 6.10199\n",
      "Iteration 41, ||alpha - alpha_prev|| = 5.06759\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 65\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m     59\u001b[0m     [custom_test_metrics_smo, custom_test_metrics_no_smo, sklearn_test_metrics],\n\u001b[0;32m     60\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     61\u001b[0m     keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom SVR (SMO)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom SVR (No SMO)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSklearn SVR\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     62\u001b[0m ))\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 交叉验证\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m custom_cv_metrics_smo \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_svr_smo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkfold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m custom_cv_metrics_no_smo \u001b[38;5;241m=\u001b[39m cross_validate(custom_svr_no_smo, X_reg, y_reg, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkfold\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, classifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m sklearn_cv_metrics \u001b[38;5;241m=\u001b[39m cross_validate(sklearn_svr, X_reg, y_reg, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkfold\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, classifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[34], line 227\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(model, X, y, cv, n_splits, classifier)\u001b[0m\n\u001b[0;32m    224\u001b[0m X_train, X_val \u001b[38;5;241m=\u001b[39m X[train_idx], X[val_idx]\n\u001b[0;32m    225\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y[train_idx], y[val_idx]\n\u001b[1;32m--> 227\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, X_val, y_val, classifier\u001b[38;5;241m=\u001b[39mclassifier)\n\u001b[0;32m    229\u001b[0m metrics_list\u001b[38;5;241m.\u001b[39mappend(metrics\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[34], line 72\u001b[0m, in \u001b[0;36mSVM.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n_samples):  \u001b[38;5;66;03m# 避免重复计算\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# 判断收敛条件\u001b[39;00m\n\u001b[0;32m     74\u001b[0m norm_diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m-\u001b[39m alpha_prev)\n",
      "Cell \u001b[1;32mIn[34], line 140\u001b[0m, in \u001b[0;36mSVM.update\u001b[1;34m(self, i, j)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[j] \u001b[38;5;241m=\u001b[39m a_j_new\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# 更新偏置项 b\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m b_i_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[i] \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m b_j_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[j] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK[j]), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e5\u001b[39m, \u001b[38;5;241m1e5\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m (b_i_new \u001b[38;5;241m+\u001b[39m b_j_new) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2169\u001b[0m, in \u001b[0;36mclip\u001b[1;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[0;32m   2104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2167\u001b[0m \n\u001b[0;32m   2168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\ml\\lib\\site-packages\\numpy\\core\\_methods.py:99\u001b[0m, in \u001b[0;36m_clip\u001b[1;34m(a, min, max, out, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m um\u001b[38;5;241m.\u001b[39mmaximum(a, \u001b[38;5;28mmin\u001b[39m, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m um\u001b[38;5;241m.\u001b[39mclip(a, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_reg = np.random.rand(200, 2) * 10\n",
    "    y_reg = 3 * X_reg[:, 0] + 2 * X_reg[:, 1] + np.random.randn(200)\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    X_reg = scaler_X.fit_transform(X_reg)\n",
    "    y_reg = scaler_y.fit_transform(y_reg.reshape(-1, 1)).ravel()\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_reg, y_reg, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    custom_svr_smo = SVM(C=1, kernel=\"linear\", task=\"regression\", use_smo=True, max_iter=500)\n",
    "    custom_svr_smo.fit(X_train, y_train)\n",
    "\n",
    "    custom_svr_no_smo = SVM(C=1, kernel=\"linear\", task=\"regression\", use_smo=False)\n",
    "    custom_svr_no_smo.fit(X_train, y_train)\n",
    "\n",
    "    sklearn_svr = SVR(C=1, kernel=\"linear\")\n",
    "    sklearn_svr.fit(X_train, y_train)\n",
    "\n",
    "    custom_train_metrics_smo = evaluate_model(custom_svr_smo, X_train, y_train, classifier=False)\n",
    "    custom_val_metrics_smo = evaluate_model(custom_svr_smo, X_val, y_val, classifier=False)\n",
    "    custom_test_metrics_smo = evaluate_model(custom_svr_smo, X_test, y_test, classifier=False)\n",
    "\n",
    "    custom_train_metrics_no_smo = evaluate_model(custom_svr_no_smo, X_train, y_train, classifier=False)\n",
    "    custom_val_metrics_no_smo = evaluate_model(custom_svr_no_smo, X_val, y_val, classifier=False)\n",
    "    custom_test_metrics_no_smo = evaluate_model(custom_svr_no_smo, X_test, y_test, classifier=False)\n",
    "\n",
    "    sklearn_train_metrics = evaluate_model(sklearn_svr, X_train, y_train, classifier=False)\n",
    "    sklearn_val_metrics = evaluate_model(sklearn_svr, X_val, y_val, classifier=False)\n",
    "    sklearn_test_metrics = evaluate_model(sklearn_svr, X_test, y_test, classifier=False)\n",
    "\n",
    "    print(\"\\nRegression Train Metrics Comparison:\")\n",
    "    print(pd.concat(\n",
    "        [custom_train_metrics_smo, custom_train_metrics_no_smo, sklearn_train_metrics],\n",
    "        axis=1,\n",
    "        keys=[\"Custom SVR (SMO)\", \"Custom SVR (No SMO)\", \"Sklearn SVR\"]\n",
    "    ))\n",
    "\n",
    "    print(\"\\nRegression Validation Metrics Comparison:\")\n",
    "    print(pd.concat(\n",
    "        [custom_val_metrics_smo, custom_val_metrics_no_smo, sklearn_val_metrics],\n",
    "        axis=1,\n",
    "        keys=[\"Custom SVR (SMO)\", \"Custom SVR (No SMO)\", \"Sklearn SVR\"]\n",
    "    ))\n",
    "\n",
    "    print(\"\\nRegression Test Metrics Comparison:\")\n",
    "    print(pd.concat(\n",
    "        [custom_test_metrics_smo, custom_test_metrics_no_smo, sklearn_test_metrics],\n",
    "        axis=1,\n",
    "        keys=[\"Custom SVR (SMO)\", \"Custom SVR (No SMO)\", \"Sklearn SVR\"]\n",
    "    ))\n",
    "\n",
    "    custom_cv_metrics_smo = cross_validate(custom_svr_smo, X_reg, y_reg, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "    custom_cv_metrics_no_smo = cross_validate(custom_svr_no_smo, X_reg, y_reg, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "    sklearn_cv_metrics = cross_validate(sklearn_svr, X_reg, y_reg, cv=\"kfold\", n_splits=5, classifier=False)\n",
    "\n",
    "    print(\"\\nRegression Cross-Validation Metrics Comparison:\")\n",
    "    print(pd.concat(\n",
    "        [custom_cv_metrics_smo, custom_cv_metrics_no_smo, sklearn_cv_metrics],\n",
    "        axis=1,\n",
    "        keys=[\"Custom SVR (SMO)\", \"Custom SVR (No SMO)\", \"Sklearn SVR\"]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer, make_regression\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MyAdaboost:\n",
    "    def __init__(self, n_estimators=50, task=\"classification\"):\n",
    "        assert task in [\"classification\", \"regression\"], \"Invalid task type! Choose 'classification' or 'regression'.\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.task = task\n",
    "        self.clfs = []\n",
    "        self.alphas = []\n",
    "        self.weights = None\n",
    "\n",
    "    def _G(self, fi, fv, direct):\n",
    "        assert direct in [\"positive\", \"negative\"], \"Direction must be 'positive' or 'negative'.\"\n",
    "\n",
    "        def _g(X):\n",
    "            if direct == \"positive\":\n",
    "                return np.where(X[:, fi] <= fv, -1, 1)\n",
    "            else:\n",
    "                return np.where(X[:, fi] > fv, -1, 1)\n",
    "\n",
    "        return _g\n",
    "\n",
    "    def _best_split(self, X, y, w):\n",
    "        best_err = float('inf')\n",
    "        best_fi, best_fv, best_direct = None, None, None\n",
    "\n",
    "        for fi in range(X.shape[1]):\n",
    "            series = X[:, fi]\n",
    "            unique_values = np.unique(series)\n",
    "\n",
    "            if len(unique_values) == 1:  \n",
    "                continue\n",
    "\n",
    "            for fv in unique_values:\n",
    "                predict = np.where(series <= fv, -1, 1)\n",
    "                err = np.sum(w * (predict != y))\n",
    "\n",
    "                if err < best_err:\n",
    "                    best_err, best_fi, best_fv, best_direct = err, fi, fv, \"positive\"\n",
    "\n",
    "                predict = -predict\n",
    "                err = np.sum(w * (predict != y))\n",
    "\n",
    "                if err < best_err:\n",
    "                    best_err, best_fi, best_fv, best_direct = err, fi, fv, \"negative\"\n",
    "\n",
    "        if best_fv is None:\n",
    "            raise ValueError(\"Failed to find a valid split point.\")\n",
    "\n",
    "        return best_err, best_fi, best_fv, best_direct\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        n_samples = len(y_train)\n",
    "        self.weights = np.ones(n_samples) / n_samples  \n",
    "        self.alphas = []  \n",
    "        self.clfs = []   \n",
    "\n",
    "        for _ in tqdm(range(self.n_estimators), desc=\"Training AdaBoost\"):\n",
    "            err, fi, fv, direct = self._best_split(X_train, y_train, self.weights)\n",
    "\n",
    "            if err == 0:\n",
    "                alpha = 1  \n",
    "            else:\n",
    "                alpha = 0.5 * np.log((1 - err) / max(err, 1e-10))  \n",
    "\n",
    "            self.alphas.append(alpha)\n",
    "            self.clfs.append(self._G(fi, fv, direct)) \n",
    "\n",
    "            predictions = self.clfs[-1](X_train)\n",
    "            if self.task == \"classification\":\n",
    "                self.weights *= np.exp(-alpha * y_train * predictions)\n",
    "            elif self.task == \"regression\":\n",
    "                residuals = y_train - predictions\n",
    "                self.weights *= np.exp(-alpha * np.abs(residuals))\n",
    "\n",
    "            self.weights /= np.sum(self.weights)  \n",
    "\n",
    "            if err == 0:\n",
    "                print(f\"Converged early at iteration {len(self.alphas)}\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_p = np.sum([self.alphas[i] * self.clfs[i](X_test) for i in range(len(self.clfs))], axis=0)\n",
    "\n",
    "        if self.task == \"classification\":\n",
    "            return np.sign(y_p)\n",
    "        elif self.task == \"regression\":\n",
    "            return y_p\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        if self.task == \"classification\":\n",
    "            return accuracy_score(y_test, y_pred)\n",
    "        elif self.task == \"regression\":\n",
    "            return r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, classifier=True):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    if classifier: \n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y, y_pred),\n",
    "            \"Precision\": precision_score(y, y_pred, average=\"weighted\"),\n",
    "            \"Recall\": recall_score(y, y_pred, average=\"weighted\"),\n",
    "            \"F1 Score\": f1_score(y, y_pred, average=\"weighted\"),\n",
    "        }\n",
    "    else: \n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y) - 1) / (len(y) - X.shape[1] - 1)\n",
    "        metrics = {\n",
    "            \"MAE\": mean_absolute_error(y, y_pred),\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": np.sqrt(mse),\n",
    "            \"R2 Square\": r2,\n",
    "            \"Adj R Square\": adj_r2,\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(metrics.items(), columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "\n",
    "def cross_validate(model, X, y, cv=\"kfold\", n_splits=5, classifier=True):\n",
    "    if cv == \"kfold\":\n",
    "        cv_splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    elif cv == \"stratified\":\n",
    "        if not classifier:\n",
    "            raise ValueError(\"Stratified K-Fold only works for classification tasks.\")\n",
    "        cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cv type. Choose from 'kfold' or 'stratified'.\")\n",
    "\n",
    "    metrics_list = []\n",
    "    for train_idx, val_idx in cv_splitter.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_val, y_val, classifier=classifier)\n",
    "        metrics_list.append(metrics.set_index(\"Metric\")[\"Value\"])\n",
    "\n",
    "    avg_metrics = pd.DataFrame(metrics_list).mean(axis=0)\n",
    "    return avg_metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training AdaBoost: 100%|██████████| 50/50 [00:05<00:00,  9.56it/s]\n",
      "f:\\anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Train Metrics Comparison:\n",
      "  Custom Model       Sklearn Model      \n",
      "        Metric Value        Metric Value\n",
      "0     Accuracy   1.0      Accuracy   1.0\n",
      "1    Precision   1.0     Precision   1.0\n",
      "2       Recall   1.0        Recall   1.0\n",
      "3     F1 Score   1.0      F1 Score   1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    breast_cancer = load_breast_cancer()\n",
    "    X_class = breast_cancer.data\n",
    "    y_class = np.where(breast_cancer.target == 0, -1, 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42)\n",
    "\n",
    "    custom_clf = MyAdaboost(n_estimators=50, task=\"classification\")\n",
    "    custom_clf.fit(X_train, y_train)\n",
    "\n",
    "    sklearn_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "    sklearn_clf.fit(X_train, y_train)\n",
    "\n",
    "    custom_train_metrics = evaluate_model(custom_clf, X_train, y_train, classifier=True)\n",
    "    sklearn_train_metrics = evaluate_model(sklearn_clf, X_train, y_train, classifier=True)\n",
    "\n",
    "    print(\"Classification Train Metrics Comparison:\")\n",
    "    print(pd.concat([custom_train_metrics, sklearn_train_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))\n",
    "\n",
    "    # X_reg, y_reg = make_regression(n_samples=200, n_features=2, noise=10, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "    # custom_reg = MyAdaboost(n_estimators=50, task=\"regression\")\n",
    "    # custom_reg.fit(X_train, y_train)\n",
    "\n",
    "    # sklearn_reg = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
    "    # sklearn_reg.fit(X_train, y_train)\n",
    "\n",
    "    # custom_train_metrics = evaluate_model(custom_reg, X_train, y_train, classifier=False)\n",
    "    # sklearn_train_metrics = evaluate_model(sklearn_reg, X_train, y_train, classifier=False)\n",
    "\n",
    "    # print(\"\\nRegression Train Metrics Comparison:\")\n",
    "    # print(pd.concat([custom_train_metrics, sklearn_train_metrics], axis=1, keys=[\"Custom Model\", \"Sklearn Model\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
